{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maryamdarei/CNN/blob/main/NEW_CNN_from_github%2C_with_GRADCAM_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2654fd3c",
      "metadata": {
        "id": "2654fd3c"
      },
      "source": [
        "It iS a package, from Githup:\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "CNN trained on a small imagenet dataset\n",
        "Imagenette is a subset of 10 easily classified classes from \n",
        "Imagenet (tench, English springer, cassette player, chain saw, church, French horn, garbage truck, gas pump, golf ball, parachute).\n",
        "https://github.com/fastai/imagenette\n",
        "Download the Imagenette dataset from Github to Imageneet folder\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "https://github.com/alexcpn/cnn_train\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5kyktviRpzmi",
        "outputId": "0e8df56b-e930-427f-999b-443b0a7056a6"
      },
      "id": "5kyktviRpzmi",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Model Data/Logs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4PJDIwfp__A",
        "outputId": "6a4d43b4-af6d-4e5c-d78a-cff836f3aa68"
      },
      "id": "Y4PJDIwfp__A",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Model Data/Logs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar xzvf imagenette2-320.tgz"
      ],
      "metadata": {
        "id": "FtlucWEKqMLa"
      },
      "id": "FtlucWEKqMLa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install models"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oIW47HdQwgPf",
        "outputId": "92af527f-4760-409e-e56d-0adefc24cfce"
      },
      "id": "oIW47HdQwgPf",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting models\n",
            "  Using cached models-0.9.3.tar.gz (16 kB)\n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/92/3c/ac1ddde60c02b5a46993bd3c6f4c66a9dbc100059da8333178ce17a22db5/models-0.9.3.tar.gz#sha256=b5aa29c6b57a667cda667dd9fbd33bbd15c14cc285e57dda64f4f4c0fd35e0ae (from https://pypi.org/simple/models/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "  Using cached models-0.9.2.tar.gz (16 kB)\n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/44/3a/8cd3c08995d89e87e74a9aa04784dcb9ade25c03250c281376ce4942d7f5/models-0.9.2.tar.gz#sha256=308be4d5cb707c63f967c591111a8675fe3eaa59c04516e5f6c7fff5e026ece0 (from https://pypi.org/simple/models/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "  Using cached models-0.9.1.tar.gz (16 kB)\n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/10/1d/dd6ae3703251520729a9203ce95c436837ca28e0ee6794538efb136ac0e4/models-0.9.1.tar.gz#sha256=78eb7e8cef14864cda9ec5c1911f3c0771671734c71945ec4f908c505d3be8f9 (from https://pypi.org/simple/models/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "  Using cached models-0.9.0.tar.gz (14 kB)\n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/05/2a/025b493fac59ba30b4ba63c15e72200b0d39a44f22a226f3d69b176a03df/models-0.9.0.tar.gz#sha256=9c16463aef8fe05e856d74c084035bac1d53936f6274444611eee7cb2ccc4556 (from https://pypi.org/simple/models/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "  Using cached models-0.8.0.tar.gz (14 kB)\n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/63/e1/b6f02f5df8e27da6a3ea3b7f61ff7ab2d17b4b98290a54799c1bd712b883/models-0.8.0.tar.gz#sha256=c2d01e2c83fb39866ad622e0dbe0e211cf694b5148550cca079411249171c747 (from https://pypi.org/simple/models/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "  Using cached models-0.7.0.tar.gz (14 kB)\n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/09/88/72909bdb09041e6adcc90944f8e9a08fb578aaa6e7699e80a9c147ff7c1f/models-0.7.0.tar.gz#sha256=e3d01ad00a6ec6831de6138a3f986c2efb7bd9355d8bb3f5b700eb2e96b6e3e7 (from https://pypi.org/simple/models/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "  Using cached models-0.4.0.tar.gz (11 kB)\n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/79/44/59bdda622faaec86c9db3f85270e29a3485936753b779c8210d3c0499ccb/models-0.4.0.tar.gz#sha256=80314c00ee1e8b19b4536107daac00b4e986f048e58d73530a00348322bc4399 (from https://pypi.org/simple/models/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "  Using cached models-0.3.0.tar.gz (11 kB)\n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/fb/dc/fd2068f4cc4f275a1a2d41a2ca6a6b3a0d49c7e86ba3a30d2fbf15731f7a/models-0.3.0.tar.gz#sha256=ce0350c757e27dcf7476d9a3f030a3f408c0b318a6b1153a83db8206766fae10 (from https://pypi.org/simple/models/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "  Using cached models-0.2.0.tar.gz (11 kB)\n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/27/62/62ebd85760e4f812753056b4763f0a866cdcbc08ee6ccc2b81a2f5fb2202/models-0.2.0.tar.gz#sha256=2f5982ef56afe9239f81f5d6a461823e0dc267c6212c06754a0863b98dad60a4 (from https://pypi.org/simple/models/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "  Using cached models-0.1.1.tar.gz (11 kB)\n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/ca/81/53baac8e352f956a23568f285a72bcd2d8cc04321022f0196de5eaf5e5e4/models-0.1.1.tar.gz#sha256=b117db9acd5a232f4333a5927385320826168514364e86d1dbaba44b01fab1a9 (from https://pypi.org/simple/models/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "  Using cached models-0.1.0.tar.gz (11 kB)\n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/ad/3b/596d1bb079039dc29b92692e59b87d7f352a15e25cca6b5b304ff3efce9a/models-0.1.0.tar.gz#sha256=0b74111b909079ef6baaa10e1619ff5aaeaace807f0bcc948650c38e81534203 (from https://pypi.org/simple/models/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "  Using cached models-0.0.5.tar.gz (6.4 kB)\n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/b9/12/3a057a76ef4063e25584e922228e82f9fb0ab20e888e97dd4d109d017207/models-0.0.5.tar.gz#sha256=5832d3221cce8918872873b7aa8436d80c829de1b7ba4ec21944c9c361a12386 (from https://pypi.org/simple/models/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "  Using cached models-0.0.4.tar.gz (6.1 kB)\n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/e6/6a/0a4cb29c5d32ddbe06080deebd1e4f8896f6de56f954d690d492e8044b91/models-0.0.4.tar.gz#sha256=4537a1b50b1efb54ff52762f6108be8afcfc194927380d948d7d983ac2557a98 (from https://pypi.org/simple/models/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "  Using cached models-0.0.3.tar.gz (6.2 kB)\n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/68/35/b990e771f13982d98385774c651c0c90baec4b4ef19c752b5e6add126d3f/models-0.0.3.tar.gz#sha256=9bb0315231b46d53df62c63321c33fc6935ca58db629e3aef09c735b781a9d8b (from https://pypi.org/simple/models/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement models (from versions: 0.0.3, 0.0.4, 0.0.5, 0.1.0, 0.1.1, 0.2.0, 0.3.0, 0.4.0, 0.7.0, 0.8.0, 0.9.0, 0.9.1, 0.9.2, 0.9.3)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for models\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "e823be0a",
      "metadata": {
        "id": "e823be0a"
      },
      "outputs": [],
      "source": [
        "\n",
        "import logging as log\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from matplotlib import ticker\n",
        "#I commented this:\n",
        "#from models import alexnet, mycnn, mycnn2, resnet\n",
        "from torch.utils.tensorboard import SummaryWriter\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EzW0nXX9B0Qm",
        "outputId": "ed43ab9a-068a-4540-9b2d-9695834efed3"
      },
      "id": "EzW0nXX9B0Qm",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Model Data/Logs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "c44ea7ba",
      "metadata": {
        "id": "c44ea7ba"
      },
      "outputs": [],
      "source": [
        "\n",
        "log.basicConfig(format=\"%(asctime)s %(message)s\", level=log.INFO)\n",
        "log.info(\"======= starting to log ========\")\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#change the log code\n",
        "log.basicConfig(filename=\"/content/drive/MyDrive/Model Data/Logs/logs.log\",format=\"%(asctime)s %(message)s\", level=log.INFO)\n",
        "log.info(\"======= starting to log ========\")\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "r1oqszqiClUe"
      },
      "id": "r1oqszqiClUe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /drive/MyDrive/Model Data/Logs/logs.log"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rP9GCZYA-hPF",
        "outputId": "629f3cc7-4e88-4d90-9794-588be4cf53f7"
      },
      "id": "rP9GCZYA-hPF",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cat: /drive/MyDrive/Model: No such file or directory\n",
            "cat: Data/Logs/logs.log: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "cd5e1510",
      "metadata": {
        "id": "cd5e1510"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define relevant variables for the ML task\n",
        "num_classes = 10\n",
        "learning_rate = 0.001\n",
        "num_epochs = 20  # actual 20 epochs\n",
        "workers = 0\n",
        "pin_memory = False\n",
        "batch_size = 64\n",
        "#I commented these:\n",
        "#torch.cuda.empty_cache()\n",
        "#torch.cuda.reset_max_memory_allocated()\n",
        "#torch.cuda.synchronize()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "637c7b51",
      "metadata": {
        "id": "637c7b51"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Device will determine whether to run the training on GPU or CPU.\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "if device.type == \"cuda\":\n",
        "    deviceid = torch.cuda.current_device()\n",
        "    log.info(f\"Gpu device {torch.cuda.get_device_name(deviceid)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbb54221",
      "metadata": {
        "id": "dbb54221"
      },
      "source": [
        "-------------------------------------------------------------------------------------------------------\n",
        "# Select the model you want to train\n",
        "-------------------------------------------------------------------------------------------------------\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "4bc27c73",
      "metadata": {
        "id": "4bc27c73"
      },
      "outputs": [],
      "source": [
        "modelname = \"alexnet_\"\n",
        "\n",
        "if modelname == \"mycnn_\":\n",
        "    # Actual image size is 432*320\n",
        "    model = mycnn.MyCNN().to(device)\n",
        "    resize_to = transforms.Resize((227, 227))\n",
        "if modelname == \"mycnn2_\":\n",
        "    # Actual image size is 432*320\n",
        "    model = mycnn2.MyCNN2().to(device)\n",
        "    resize_to = transforms.Resize((227, 227))\n",
        "#if modelname == \"alexnet_\":\n",
        "    # Alexnet model works well for CIFAR-10 when input is scaled to 227x227\n",
        "    model = alexnet.AlexNet().to(device)\n",
        "    resize_to = transforms.Resize((227, 227))\n",
        "\n",
        "if modelname == \"RestNet50_\":\n",
        "    model = resnet.ResNet50(img_channel=3, num_classes=10).to(device)\n",
        "    # resizing lower to keep it in memory\n",
        "    resize_to = transforms.Resize((227, 227))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "def list_files(dir):\n",
        "    r = []\n",
        "    for root, dirs, files in os.walk(dir):\n",
        "        for name in files:\n",
        "            r.append(os.path.join(root, name))\n",
        "    return r\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    paths = list_files(\"imagenette2-320/val/dogs50A-val\")\n",
        "    problem_files =[]\n",
        "    for path in paths:\n",
        "        with open(path, 'rb') as file:\n",
        "            try:\n",
        "                img = Image.open(file).load()\n",
        "            except Exception as e:\n",
        "                print(e)\n",
        "                print(path)\n",
        "                problem_files.append(path)\n"
      ],
      "metadata": {
        "id": "LyGRTPLZzIQi"
      },
      "id": "LyGRTPLZzIQi",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "bbfa0e7f",
      "metadata": {
        "id": "bbfa0e7f"
      },
      "outputs": [],
      "source": [
        "\n",
        "# -------------------------------------------------------------------------------------------------------\n",
        "# Load the data from image folder\n",
        "# -------------------------------------------------------------------------------------------------------\n",
        "\n",
        "data_dir = \"/content/drive/MyDrive/Model Data/imagenette/imagenette2-320\"\n",
        "train_dir = os.path.join(data_dir, \"train\")\n",
        "normalize_transform = transforms.Normalize(\n",
        "    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
        ")\n",
        "\n",
        "# I changed the code for resize the image:\n",
        "train_transforms = transforms.Compose(\n",
        "    [\n",
        "        \n",
        "        transforms.Resize((277,277)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomGrayscale(0.5),\n",
        "        transforms.ToTensor(),\n",
        "        normalize_transform,\n",
        "    ]\n",
        ")\n",
        "\n",
        "val_dir = os.path.join(data_dir, \"val\")\n",
        "\n",
        "val_transforms = transforms.Compose(\n",
        "    [transforms.Resize((277,277)), transforms.ToTensor(), normalize_transform]\n",
        ")\n",
        "\n",
        "\n",
        "train_dataset = torchvision.datasets.ImageFolder(train_dir, train_transforms)\n",
        "val_dataset = torchvision.datasets.ImageFolder(val_dir, val_transforms)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "b8987a4e",
      "metadata": {
        "id": "b8987a4e"
      },
      "outputs": [],
      "source": [
        "\n",
        "# -----------------------------------------------------------------------------------------------------\n",
        "# Order the categories as per how Dataloader loads it\n",
        "# -----------------------------------------------------------------------------------------------------\n",
        "\n",
        "foldername_to_class = {\n",
        "    \"n02102040\": \"dog\",\n",
        "    \"n01440764\": \"tench\",\n",
        "    \"n02979186\": \"cassette player\",\n",
        "    \"n03000684\": \"chain saw\",\n",
        "    \"n03028079\": \"church\",\n",
        "    \"n03394916\": \"French horn\",\n",
        "    \"n03417042\": \"garbage truck\",\n",
        "    \"n03425413\": \"gas pump\",\n",
        "    \"n03445777\": \"golf ball\",\n",
        "    \"n03888257\": \"parachute\",\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "88f0ce32",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88f0ce32",
        "outputId": "f4f49840-7bb2-4c95-c138-b0a5c6cd8e0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image to Folder Index {'n01440764': 0, 'n02102040': 1, 'n02979186': 2, 'n03000684': 3, 'n03028079': 4, 'n03394916': 5, 'n03417042': 6, 'n03425413': 7, 'n03445777': 8, 'n03888257': 9}\n"
          ]
        }
      ],
      "source": [
        "# sort as value to fit the directory order to labels to be sure\n",
        "print(\"Image to Folder Index\", train_dataset.class_to_idx)\n",
        "sorted_vals = dict(sorted(train_dataset.class_to_idx.items(), key=lambda item: item[1]))\n",
        "categories = []\n",
        "for key in sorted_vals:\n",
        "    classname = foldername_to_class[key]\n",
        "    categories.append(classname)\n",
        "\n",
        "log.info(f\"Categories {categories}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "0b0007a5",
      "metadata": {
        "id": "0b0007a5"
      },
      "outputs": [],
      "source": [
        "# -------------------------------------------------------------------------------------------------------\n",
        "# Initialise the data loaders\n",
        "# -------------------------------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "# ImageFile.LOAD_TRUNCATED_IMAGES = True # Use the data_checker.py and remove bad files instead of using this\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,  # IMPORTANT otherwise the data is not shuffled\n",
        "    num_workers=workers,\n",
        "    pin_memory=pin_memory,\n",
        "    sampler=None,\n",
        ")\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=workers,\n",
        "    pin_memory=pin_memory,\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#I added this line for running the model:\n",
        "model = torchvision.models.resnet50()\n",
        "model.to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1n-y_baT45FT",
        "outputId": "a8c5faad-593d-4ef6-e8ce-1192714df2e4"
      },
      "id": "1n-y_baT45FT",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (4): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (5): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "edHC9hB1wpn9"
      },
      "id": "edHC9hB1wpn9"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "8ecd5637",
      "metadata": {
        "id": "8ecd5637"
      },
      "outputs": [],
      "source": [
        "\n",
        "# initialize our optimizer and loss function\n",
        "opt = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "lossFn = nn.CrossEntropyLoss()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "ed2d719a",
      "metadata": {
        "id": "ed2d719a"
      },
      "outputs": [],
      "source": [
        "\n",
        "# -------------------------------------------------------------------------------------------------------\n",
        "# Train the model\n",
        "# -------------------------------------------------------------------------------------------------------\n",
        "\n",
        "for images, labels in train_loader:\n",
        "    log.info(f\"Shape of X [N, C, H, W]: {images.shape}\")\n",
        "    log.info(f\"Shape of y: {labels.shape} {labels.dtype}\")\n",
        "    # test one flow\n",
        "    # pred = model(x)\n",
        "    # loss = lossFn(pred, y)\n",
        "    break\n",
        "total_step = len(train_loader)\n",
        "log.info(f\"Total steps: {total_step}\")\n",
        "\n",
        "stepsize = total_step // 100\n",
        "if stepsize < 10:\n",
        "    stepsize = 10\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "af1df759",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "af1df759",
        "outputId": "2000b748-064b-4be3-f766-c5d01776d405"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Loss: 1.3860, TrainAccuracy: 60.6557, averageloss: 1.4497, averageacc: 51.8034 \n",
            "Epoch [2/20], Loss: 0.8543, TrainAccuracy: 73.7705, averageloss: 1.2265, averageacc: 59.7995 \n",
            "Epoch [3/20], Loss: 1.0099, TrainAccuracy: 68.8525, averageloss: 1.0942, averageacc: 64.3905 \n",
            "Epoch [4/20], Loss: 0.8686, TrainAccuracy: 72.1311, averageloss: 1.0238, averageacc: 67.0098 \n",
            "Epoch [5/20], Loss: 0.8936, TrainAccuracy: 70.4918, averageloss: 0.9356, averageacc: 69.5536 \n",
            "Epoch [6/20], Loss: 0.9409, TrainAccuracy: 65.5738, averageloss: 0.8667, averageacc: 71.7796 \n",
            "Epoch [7/20], Loss: 0.6046, TrainAccuracy: 83.6066, averageloss: 0.8063, averageacc: 73.9391 \n",
            "Epoch [8/20], Loss: 0.6112, TrainAccuracy: 78.6885, averageloss: 0.7523, averageacc: 75.5528 \n",
            "Epoch [9/20], Loss: 0.6616, TrainAccuracy: 72.1311, averageloss: 0.7209, averageacc: 76.6909 \n",
            "Epoch [10/20], Loss: 0.6847, TrainAccuracy: 78.6885, averageloss: 0.6643, averageacc: 78.4033 \n",
            "Epoch [11/20], Loss: 0.5495, TrainAccuracy: 81.9672, averageloss: 0.6194, averageacc: 79.9880 \n",
            "Epoch [12/20], Loss: 0.4102, TrainAccuracy: 88.5246, averageloss: 0.5471, averageacc: 81.9643 \n",
            "Epoch [13/20], Loss: 0.5927, TrainAccuracy: 86.8852, averageloss: 0.5163, averageacc: 83.0089 \n",
            "Epoch [14/20], Loss: 0.5819, TrainAccuracy: 81.9672, averageloss: 0.4858, averageacc: 84.1793 \n",
            "Epoch [15/20], Loss: 0.4512, TrainAccuracy: 85.2459, averageloss: 0.4357, averageacc: 85.8695 \n",
            "Epoch [16/20], Loss: 0.5872, TrainAccuracy: 85.2459, averageloss: 0.4441, averageacc: 85.5528 \n",
            "Epoch [17/20], Loss: 0.4858, TrainAccuracy: 80.3279, averageloss: 0.4083, averageacc: 86.4697 \n",
            "Epoch [18/20], Loss: 0.4216, TrainAccuracy: 88.5246, averageloss: 0.3601, averageacc: 87.8236 \n",
            "Epoch [19/20], Loss: 0.4057, TrainAccuracy: 83.6066, averageloss: 0.3194, averageacc: 89.5113 \n",
            "Epoch [20/20], Loss: 0.2153, TrainAccuracy: 91.8033, averageloss: 0.2996, averageacc: 89.9151 \n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Write training matrics to Tensorboard\n",
        "writer = SummaryWriter()\n",
        "\n",
        "# loop over our epochs\n",
        "for epoch in range(0, num_epochs):\n",
        "    # set the model in training mode\n",
        "    model.train()  # IMPORTANT otherwise the model is not in training mode\n",
        "    # initialize the total training and validation loss\n",
        "    totalTrainLoss = 0\n",
        "    totalValLoss = 0\n",
        "    # initialize the number of correct predictions in the training\n",
        "    # and validation step\n",
        "    trainAccuracy = 0\n",
        "    totalTrainAccuracy = 0\n",
        "    valCorrect = 0\n",
        "\n",
        "    # loop over the training set\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "\n",
        "        try:\n",
        "            # Train in auto-mode with 16 bit mode\n",
        "            # with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
        "            # Train in normal mode\n",
        "            with torch.autocast(device_type=\"cuda\", dtype=torch.float32):\n",
        "                # send the input to the device\n",
        "                (images, labels) = (images.to(device), labels.to(device))\n",
        "                # perform a forward pass and calculate the training loss\n",
        "                outputs = model(images)\n",
        "                # output is float16 because linear layers autocast to float16.\n",
        "                # assert outputs.dtype is torch.float16 or 64\n",
        "\n",
        "                loss = lossFn(outputs, labels)\n",
        "                # zero out the gradients, perform the backpropagation step,\n",
        "                # and update the weights\n",
        "                writer.add_scalar(\"Loss/train\", loss,  (epoch * total_step)+(i+1))\n",
        "                opt.zero_grad()  # IMPORTANT otherwise the gradients of previous batches are not zeroed out\n",
        "        except Exception as e:\n",
        "            log.error(f\"Exception in data processing- skip and continue = {e}\")\n",
        "        loss.backward()\n",
        "        totalTrainLoss += loss\n",
        "        opt.step()\n",
        "        # Get the predicted values\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        trainAccuracy = (predicted == labels).float().sum().item()\n",
        "        trainAccuracy = 100 * trainAccuracy / labels.size(0)\n",
        "        writer.add_scalar(\"Accuracy/train\", trainAccuracy,(epoch * total_step)+(i+1))\n",
        "        totalTrainAccuracy += trainAccuracy\n",
        "        # if (i // stepsize) % 10 == 0:\n",
        "        log.info(\n",
        "            \"Epoch [{}/{}], Step [{}/{}], Loss: {:.4f} Accuracy: {:.4f}\".format(\n",
        "                epoch + 1, num_epochs, i + 1, total_step, loss, trainAccuracy\n",
        "            )\n",
        "        )\n",
        "\n",
        "    \n",
        "\n",
        "    avgTrainLoss = totalTrainLoss / len(train_loader)\n",
        "    avgAccuracy = totalTrainAccuracy / len(train_loader)\n",
        "    log.info(\n",
        "        \"--->Epoch [{}/{}], Average Loss: {:.4f} Average Accuracy: {:.4f}\".format(\n",
        "            epoch + 1, num_epochs, avgTrainLoss, avgAccuracy\n",
        "        )\n",
        "    )\n",
        "\n",
        "    print('Epoch [{}/{}], Loss: {:.4f}, TrainAccuracy: {:.4f}, averageloss: {:.4f}, averageacc: {:.4f} '.format(epoch+1, num_epochs, loss, trainAccuracy, avgTrainLoss, avgAccuracy))\n",
        "\n",
        "    # End Epoch loop\n",
        "writer.flush()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "27ba9c01",
      "metadata": {
        "id": "27ba9c01"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Save the model\n",
        "path = \"/content/drive/MyDrive/Model Data/saved_models\"\n",
        "model_save_name = path + modelname + datetime.now().strftime(\"%H:%M_%B%d%Y\")\n",
        "torch.save(model.state_dict(), model_save_name + \".pth\")\n",
        "log.info(f\"Model {modelname} saved as {model_save_name}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "f1831a49",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 784
        },
        "id": "f1831a49",
        "outputId": "5be05b6a-cf26-413c-830c-61ee62a310e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['tench', 'dog', 'cassette player', 'chain saw', 'church', 'French horn', 'garbage truck', 'gas pump', 'golf ball', 'parachute']\n",
            "\t326.0\t5.0\t10.0\t13.0\t6.0\t0.0\t1.0\t0.0\t19.0\t7.0\ttench\n",
            "\t7.0\t350.0\t6.0\t15.0\t1.0\t0.0\t1.0\t2.0\t5.0\t8.0\tdog\n",
            "\t3.0\t2.0\t277.0\t18.0\t3.0\t1.0\t7.0\t31.0\t13.0\t2.0\tcassette player\n",
            "\t19.0\t28.0\t10.0\t243.0\t10.0\t2.0\t11.0\t10.0\t26.0\t27.0\tchain saw\n",
            "\t4.0\t2.0\t6.0\t8.0\t354.0\t0.0\t3.0\t14.0\t7.0\t11.0\tchurch\n",
            "\t10.0\t21.0\t18.0\t60.0\t13.0\t177.0\t35.0\t23.0\t16.0\t21.0\tFrench horn\n",
            "\t2.0\t5.0\t14.0\t16.0\t12.0\t0.0\t287.0\t34.0\t6.0\t13.0\tgarbage truck\n",
            "\t2.0\t5.0\t28.0\t22.0\t16.0\t0.0\t10.0\t321.0\t6.0\t9.0\tgas pump\n",
            "\t12.0\t6.0\t40.0\t7.0\t8.0\t0.0\t2.0\t2.0\t304.0\t18.0\tgolf ball\n",
            "\t6.0\t8.0\t3.0\t7.0\t8.0\t0.0\t2.0\t4.0\t18.0\t334.0\tparachute\n",
            "---------------------------------------\n",
            "Accuracy/precision from confusion matrix is 0.76\n",
            "---------------------------------------\n",
            "---Accuracy for class tench = 0.84\n",
            "---Accuracy for class dog = 0.89\n",
            "---Accuracy for class cassette player = 0.78\n",
            "---Accuracy for class chain saw = 0.63\n",
            "---Accuracy for class church = 0.87\n",
            "---Accuracy for class French horn = 0.45\n",
            "---Accuracy for class garbage truck = 0.74\n",
            "---Accuracy for class gas pump = 0.77\n",
            "---Accuracy for class golf ball = 0.76\n",
            "---Accuracy for class parachute = 0.86\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Generate the Confusion Matrix\n",
        "\n",
        "confusion_matrix = np.zeros((len(categories), len(categories)))\n",
        "\n",
        "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
        "with torch.no_grad():\n",
        "    model.eval()  # IMPORTANT set model to eval mode before inference\n",
        "\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # ------------------------------------------------------------------------------------------\n",
        "        # Predict for the batch of images\n",
        "        # ------------------------------------------------------------------------------------------\n",
        "        outputs = model(\n",
        "            images\n",
        "        )  # Outputs= torch.Size([64, 10]) Probability of each of the 10 classes\n",
        "        _, predicted = torch.max(\n",
        "            outputs.data, 1\n",
        "        )  # get the class with the highest Probability out Given 1 per image # predicted= torch.Size([64])\n",
        "        # ------------------------------------------------------------------------------------------\n",
        "        #  Lets check also which classes are wrongly predicted with other classes  to create a MultiClass confusion matrix\n",
        "        # ------------------------------------------------------------------------------------------\n",
        "\n",
        "        mask = predicted != labels  # Wrongly predicted\n",
        "        wrong_predicted = torch.masked_select(predicted, mask)\n",
        "        wrong_labels = torch.masked_select(labels, mask)\n",
        "        wrongly_zipped = zip(wrong_labels, wrong_predicted)\n",
        "\n",
        "        mask = predicted == labels  # Rightly predicted\n",
        "        rightly_predicted = torch.masked_select(predicted, mask)\n",
        "        right_labels = rightly_predicted  # same torch.masked_select(labels,mask)\n",
        "        rightly_zipped = zip(right_labels, rightly_predicted)\n",
        "\n",
        "        # Note that this is for a single batch - add to the list associated with class\n",
        "        for _, j in enumerate(wrongly_zipped):\n",
        "            k = j[0].item()  # label\n",
        "            l = j[1].item()  # predicted\n",
        "            confusion_matrix[k][l] += 1\n",
        "\n",
        "        # Note that this is for a single batch - add to the list associated with class\n",
        "        for _, j in enumerate(rightly_zipped):\n",
        "            k = j[0].item()  # label\n",
        "            l = j[1].item()  # predicted\n",
        "            confusion_matrix[k][l] += 1\n",
        "\n",
        "    # print(\"Confusion Matrix1=\\n\",confusion_matrix)\n",
        "    # ------------------------------------------------------------------------------------------\n",
        "    # Print Confusion matrix in Pretty print format\n",
        "    # ------------------------------------------------------------------------------------------\n",
        "    print(categories)\n",
        "    for i in range(len(categories)):\n",
        "        for j in range(len(categories)):\n",
        "            print(f\"\\t{confusion_matrix[i][j]}\", end=\"\")\n",
        "        print(f\"\\t{categories[i]}\\n\", end=\"\")\n",
        "    # ------------------------------------------------------------------------------------------\n",
        "    # Calculate Accuracy per class\n",
        "    # ------------------------------------------------------------------------------------------\n",
        "    print(\"---------------------------------------\")\n",
        "    print(\n",
        "        f\"Accuracy/precision from confusion matrix is {round(confusion_matrix.trace()/confusion_matrix.sum(),2)}\"\n",
        "    )\n",
        "    print(\"---------------------------------------\")\n",
        "    for i in range(len(categories)):\n",
        "        print(\n",
        "            f\"---Accuracy for class {categories[i]} = {round(confusion_matrix[i][i]/confusion_matrix[i].sum(),2)}\"\n",
        "        )\n",
        "\n",
        "    # ---------------------------------------------------\n",
        "    # Plot this in a good figure\n",
        "    # ---------------------------------------------------\n",
        "\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111)\n",
        "    plt.title(\"Confusion Matrix\", fontsize=18)\n",
        "    ax.matshow(confusion_matrix, cmap=plt.cm.Blues, alpha=0.7)\n",
        "    ax.set_xticklabels([\"\"] + categories, rotation=90)\n",
        "    ax.set_yticklabels([\"\"] + categories)\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    for i in range(confusion_matrix.shape[0]):\n",
        "        for j in range(confusion_matrix.shape[1]):\n",
        "            ax.text(\n",
        "                x=j,\n",
        "                y=i,\n",
        "                s=int(confusion_matrix[i, j]),\n",
        "                va=\"center\",\n",
        "                ha=\"center\",\n",
        "                size=\"xx-small\",\n",
        "            )\n",
        "            if i == j:\n",
        "                acc = round(confusion_matrix[i][i] / confusion_matrix[i].sum(), 2)\n",
        "                ax.text(\n",
        "                    x=len(categories) + 1,\n",
        "                    y=i,\n",
        "                    s=acc,\n",
        "                    va=\"center\",\n",
        "                    ha=\"center\",\n",
        "                    size=\"xx-small\",\n",
        "                )\n",
        "    plt.savefig(model_save_name + \"_cm.jpg\")\n",
        "\n",
        "    # correct = 0\n",
        "    # total = 0\n",
        "    # for images, labels in train_loader:\n",
        "    #     images = images.to(device)\n",
        "    #     labels = labels.to(device)\n",
        "    #     outputs = model(images)\n",
        "    #     _, predicted = torch.max(outputs.data, 1)\n",
        "    #     total += labels.size(0)\n",
        "    #     correct += (predicted == labels).float().sum().item()\n",
        "    # # this is not really not needed- but just to cross check if what we calculated during training is accurate\n",
        "    # print(\n",
        "    #     \"Accuracy of the network on the {} Train images: {} %\".format(\n",
        "    #         total, 100 * correct / total\n",
        "    #     )\n",
        "    # )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"\"\"\n",
        "Pre trained  model from tutorial modified from\n",
        "https://pytorch.org/hub/pytorch_vision_alexnet/\n",
        "And for imagenette small dataset\n",
        "Imagenet (tench, English springer, cassette player, chain saw, church, French horn, garbage truck, gas pump, golf ball, parachute).\n",
        "https://github.com/fastai/imagenette\n",
        "Load the Pre-trained models generated from test4_cnn_imagenet_small.py in the same folder\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Y7BjSa1zebVN"
      },
      "id": "Y7BjSa1zebVN"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from importlib.resources import path\n",
        "from PIL import Image\n",
        "from torchvision import transforms,datasets\n",
        "import torch\n",
        "#from models import resnet, alexnet, mycnn, mycnn2\n",
        "import os\n"
      ],
      "metadata": {
        "id": "2vs5F_OqedWK"
      },
      "id": "2vs5F_OqedWK",
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhpvghH-jGHb",
        "outputId": "ca1993c7-96e3-4629-c347-da5000825cdf"
      },
      "id": "mhpvghH-jGHb",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Model Data/Logs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "test_images = ['test-tench.jpg','fish_boy.jpg','test-church.jpg','test-garbagetruck.jpg','test-truck.jpg','test-dog.jpg','train_dog.png',\n",
        "\"test-englishspringer.jpg\",\"test_dogcartoon.jpg\",\"test_chaingsaw.jpg\",\"test_chainsawtrain.jpg\",\"test_frenchhorn.jpg\",\n",
        "\"test_frenchhorntrain.jpg\",\"test-golfball.jpg\"]\n",
        "\n",
        "##/content/drive/MyDrive/Model Data/test-images\n",
        "#/content/drive/MyDrive/Model Data/imagenette/imagenette2-320/train\n",
        "\n",
        "data_dir = \"../test-images\"\n",
        "train_dir = os.path.join(data_dir, \"../imagenette/imagenette2-320/train\")\n",
        "train_dataset = datasets.ImageFolder(train_dir,[])\n"
      ],
      "metadata": {
        "id": "KevslBk5eiTc"
      },
      "id": "KevslBk5eiTc",
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#-----------------------------------------------------------------------------------------------------\n",
        "# Order the categories as per how Dataloader loads it\n",
        "#-----------------------------------------------------------------------------------------------------\n",
        "\n",
        "foldername_to_class = { 'n02102040' : \"dog\",\n",
        "                        'n01440764': \"tench\",\n",
        "                        'n02979186': \"cassette player\", \n",
        "                        'n03000684': \"chain saw\",\n",
        "                        'n03028079': \"church\",\n",
        "                        'n03394916': \"French horn\",\n",
        "                        'n03417042': \"garbage truck\",\n",
        "                        'n03425413': \"gas pump\",\n",
        "                        'n03445777':  \"golf ball\",\n",
        "                        'n03888257': \"parachute\" }"
      ],
      "metadata": {
        "id": "9Df3Mi7cemWR"
      },
      "id": "9Df3Mi7cemWR",
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sort as value to fit the directory order to labels to be sure\n",
        "print(\"Image to Folder Index\",train_dataset.class_to_idx)\n",
        "sorted_vals = dict(sorted(train_dataset.class_to_idx.items(), key=lambda item: item[1]))\n",
        "categories =[]\n",
        "for key in sorted_vals:\n",
        "    classname = foldername_to_class[key]\n",
        "    categories.append(classname)\n",
        "\n",
        "print(\"Categories\",categories)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YC5otaCceqhD",
        "outputId": "a08d6f96-3525-4d9d-b3b7-23a97f2650cc"
      },
      "id": "YC5otaCceqhD",
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image to Folder Index {'n01440764': 0, 'n02102040': 1, 'n02979186': 2, 'n03000684': 3, 'n03028079': 4, 'n03394916': 5, 'n03417042': 6, 'n03425413': 7, 'n03445777': 8, 'n03888257': 9}\n",
            "Categories ['tench', 'dog', 'cassette player', 'chain saw', 'church', 'French horn', 'garbage truck', 'gas pump', 'golf ball', 'parachute']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Choose a saved Model - assign the name you want to test with\n",
        "# (assuming that you have trained the models)\n",
        "## I commented this:\n",
        "#modelname = \"resnet50\"\n",
        "\n",
        "#if modelname == \"mycnn\":\n",
        " #   model = mycnn.MyCNN()\n",
        "  #  path = \"mycnn_18:07_October142022.pth\" \n",
        "  #  resize_to = transforms.Resize((227, 227))\n",
        "#if modelname == \"mycnn2\":\n",
        " #   model = mycnn2.MyCNN2()\n",
        "  #  path =\"mycnn2_16:43_October182022.pth\"\n",
        "   # resize_to = transforms.Resize((227, 227))\n",
        "#if modelname == \"alexnet\":\n",
        " #   model = alexnet.AlexNet()\n",
        "  #  path = \"alexnet_15:08_August082022.pth\"\n",
        "  #  resize_to = transforms.Resize((227, 227))\n",
        "#if modelname == \"resnet50\":\n",
        " #   model = resnet.ResNet50(img_channel=3, num_classes=10)\n",
        " #   path =\"RestNet50_11:45_November072022.pth\"\n",
        " #   resize_to = transforms.Resize((227, 227))\n",
        "\n",
        "#path = \"cnn/saved_models/\" +path\n",
        "resize_to = transforms.Resize((227, 227))\n",
        "#model.load_state_dict(torch.load(path))\n",
        "model.eval()\n",
        "\n",
        "for filename in test_images:\n",
        "    input_image = Image.open('../test-images/'+filename)\n",
        "    preprocess = transforms.Compose(\n",
        "        [\n",
        "            resize_to,\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(\n",
        "                mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
        "            ),  # IMPORTANT: normalize for pretrained models\n",
        "        ]\n",
        "    )\n",
        "    input_tensor = preprocess(input_image)\n",
        "    input_batch = input_tensor.unsqueeze(0)  # create a mini-batch as expected by the model\n",
        "\n",
        "    # move the input and model to GPU for speed if available\n",
        "    if torch.cuda.is_available():\n",
        "        input_batch = input_batch.to(\"cuda\")\n",
        "        model.to(\"cuda\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(input_batch)\n",
        "    # The output has unnormalized scores. To get probabilities, you can run a softmax on it.\n",
        "    probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
        "    # print(probabilities)\n",
        "    print(f\"Detecting for class {filename} model {modelname}\")\n",
        "    print(\"--------------------------------\")\n",
        "    # Show top categories per image\n",
        "    top5_prob, top5_catid = torch.topk(probabilities, 2)\n",
        "    for i in range(top5_prob.size(0)):\n",
        "        print(categories[top5_catid[i]], top5_prob[i].item())\n",
        "    print(\"--------------------------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxsfR00reuyD",
        "outputId": "3bcda0c5-79bc-4dff-cfc3-3dca07f899d9"
      },
      "id": "wxsfR00reuyD",
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detecting for class test-tench.jpg model resnet50\n",
            "--------------------------------\n",
            "tench 1.0\n",
            "parachute 2.272248877943639e-08\n",
            "--------------------------------\n",
            "Detecting for class fish_boy.jpg model resnet50\n",
            "--------------------------------\n",
            "tench 0.9134923219680786\n",
            "golf ball 0.08605096489191055\n",
            "--------------------------------\n",
            "Detecting for class test-church.jpg model resnet50\n",
            "--------------------------------\n",
            "church 0.9713032245635986\n",
            "garbage truck 0.02119547314941883\n",
            "--------------------------------\n",
            "Detecting for class test-garbagetruck.jpg model resnet50\n",
            "--------------------------------\n",
            "garbage truck 0.9934582114219666\n",
            "golf ball 0.0031143142841756344\n",
            "--------------------------------\n",
            "Detecting for class test-truck.jpg model resnet50\n",
            "--------------------------------\n",
            "tench 0.6761359572410583\n",
            "garbage truck 0.3192801773548126\n",
            "--------------------------------\n",
            "Detecting for class test-dog.jpg model resnet50\n",
            "--------------------------------\n",
            "dog 0.9888808727264404\n",
            "golf ball 0.01111255120486021\n",
            "--------------------------------\n",
            "Detecting for class train_dog.png model resnet50\n",
            "--------------------------------\n",
            "golf ball 0.6792946457862854\n",
            "parachute 0.20628774166107178\n",
            "--------------------------------\n",
            "Detecting for class test-englishspringer.jpg model resnet50\n",
            "--------------------------------\n",
            "dog 0.9448858499526978\n",
            "parachute 0.05435674265027046\n",
            "--------------------------------\n",
            "Detecting for class test_dogcartoon.jpg model resnet50\n",
            "--------------------------------\n",
            "golf ball 0.914223313331604\n",
            "parachute 0.05835423246026039\n",
            "--------------------------------\n",
            "Detecting for class test_chaingsaw.jpg model resnet50\n",
            "--------------------------------\n",
            "golf ball 0.8291880488395691\n",
            "garbage truck 0.1254543662071228\n",
            "--------------------------------\n",
            "Detecting for class test_chainsawtrain.jpg model resnet50\n",
            "--------------------------------\n",
            "parachute 0.7404531240463257\n",
            "chain saw 0.1965794414281845\n",
            "--------------------------------\n",
            "Detecting for class test_frenchhorn.jpg model resnet50\n",
            "--------------------------------\n",
            "parachute 0.9980319142341614\n",
            "chain saw 0.0011340832570567727\n",
            "--------------------------------\n",
            "Detecting for class test_frenchhorntrain.jpg model resnet50\n",
            "--------------------------------\n",
            "chain saw 0.5551360249519348\n",
            "parachute 0.32426562905311584\n",
            "--------------------------------\n",
            "Detecting for class test-golfball.jpg model resnet50\n",
            "--------------------------------\n",
            "golf ball 0.9947880506515503\n",
            "parachute 0.004976343363523483\n",
            "--------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Utility to check Precision and Recall of a  trained model Author - Alex Punnen**"
      ],
      "metadata": {
        "id": "DNHyy9gEfiJM"
      },
      "id": "DNHyy9gEfiJM"
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import ticker\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import logging as log\n",
        "#from models import resnet, alexnet, mycnn, mycnn2\n",
        "import os\n",
        "import sklearn.metrics as skmc #this has confusion matrix but need to give all in a shot ?\n"
      ],
      "metadata": {
        "id": "NZ_utDoZfjVn"
      },
      "id": "NZ_utDoZfjVn",
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log.basicConfig(format=\"%(asctime)s %(message)s\", level=log.INFO)\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "5RW8JlY5f1y6"
      },
      "id": "5RW8JlY5f1y6",
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Device will determine whether to run the training on GPU or CPU.\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "if device.type == \"cuda\":\n",
        "    deviceid = torch.cuda.current_device()\n",
        "    log.info(f\"Gpu device {torch.cuda.get_device_name(deviceid)}\")\n"
      ],
      "metadata": {
        "id": "jukP9e4gf_xl"
      },
      "id": "jukP9e4gf_xl",
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Choose a saved Model - assign the name you want to test with\n",
        "# (assuming that you have trained the models)\n",
        "# I commented this:\n",
        "#modelname = \"resnet50\"\n",
        "    \n",
        "#if modelname == \"mycnn\":\n",
        "#    model = mycnn.MyCNN()\n",
        "#    path =  \"mycnn_11:49_October302022.pth\" \n",
        "#    resize_to = transforms.Resize((150, 150))\n",
        "#if modelname == \"mycnn2\":\n",
        "#    model = mycnn2.MyCNN2()\n",
        "#    path =\"mycnn2_16:43_October182022.pth\"\n",
        "#    resize_to = transforms.Resize((227, 227))\n",
        "#if modelname == \"alexnet\":\n",
        "#    model = alexnet.AlexNet()\n",
        "#    path = \"./alexnet_15:08_August082022.pth\"\n",
        "#    resize_to = transforms.Resize((227, 227))\n",
        "#if modelname == \"resnet50\":\n",
        "#    model = resnet.ResNet50(img_channel=3, num_classes=10)\n",
        "#    path = \"./RestNet50_11:43_October072022.pth\"   # trained with more dog images from imagenet\n",
        "#    path =\"./RestNet50_11:45_November072022.pth\"\n",
        "#    resize_to = transforms.Resize((227, 227))\n",
        "\n",
        "#path = \"cnn/saved_models/\" +path\n",
        "#model.load_state_dict(torch.load(path))\n",
        "model.eval()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tV0p90TRf_1j",
        "outputId": "c69e89a2-7810-4083-b74d-f676e3ea02a1"
      },
      "id": "tV0p90TRf_1j",
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (4): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (5): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------------------------------------------------------------------\n",
        "# Load the data from image folder\n",
        "# -------------------------------------------------------------------------------------------------------\n",
        "#/content/drive/MyDrive/Model Data/imagenette/imagenette2-320\n",
        "data_dir = \"../imagenette/imagenette2-320\"\n",
        "\n",
        "\n",
        "train_dir = os.path.join(data_dir, \"train\")\n",
        "normalize_transform = transforms.Normalize(\n",
        "    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
        ")\n",
        "\n",
        "\n",
        "train_transforms = transforms.Compose(\n",
        "    [resize_to, \n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomGrayscale(),\n",
        "    transforms.ToTensor(), normalize_transform]\n",
        ")\n",
        "\n",
        "val_dir = os.path.join(data_dir, \"val\")\n",
        "\n",
        "val_transforms = transforms.Compose(\n",
        "    [resize_to, transforms.ToTensor(), normalize_transform]\n",
        ")\n",
        "\n",
        "\n",
        "train_dataset = torchvision.datasets.ImageFolder(train_dir, train_transforms)\n",
        "\n",
        "val_dataset = torchvision.datasets.ImageFolder(val_dir, val_transforms)\n"
      ],
      "metadata": {
        "id": "1tNJtNXef_4y"
      },
      "id": "1tNJtNXef_4y",
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#-----------------------------------------------------------------------------------------------------\n",
        "# Order the categories as per how Dataloader loads it\n",
        "#-----------------------------------------------------------------------------------------------------\n",
        "\n",
        "foldername_to_class = { 'n02102040' : \"dog\",\n",
        "                        'n01440764': \"tench\",\n",
        "                        'n02979186': \"cassette player\", \n",
        "                        'n03000684': \"chain saw\",\n",
        "                        'n03028079': \"church\",\n",
        "                        'n03394916': \"French horn\",\n",
        "                        'n03417042': \"garbage truck\",\n",
        "                        'n03425413': \"gas pump\",\n",
        "                        'n03445777':  \"golf ball\",\n",
        "                        'n03888257': \"parachute\" }\n",
        "\n",
        "# Imagenette classes - labels for better description\n",
        "categories_ref = [\n",
        "    \"English springer\",\n",
        "    \"tench\",\n",
        "    \"cassette player\",\n",
        "    \"chain saw\",\n",
        "    \"church\",\n",
        "    \"French horn\",\n",
        "    \"garbage truck\",\n",
        "    \"gas pump\",\n",
        "    \"golf ball\",\n",
        "    \"parachute\",\n",
        "]\n"
      ],
      "metadata": {
        "id": "1PDnIwjmgXZM"
      },
      "id": "1PDnIwjmgXZM",
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sort as value to fit the directory order to labels to be sure\n",
        "print(\"Image to Folder Index\",train_dataset.class_to_idx)\n",
        "sorted_vals = dict(sorted(train_dataset.class_to_idx.items(), key=lambda item: item[1]))\n",
        "categories =[]\n",
        "for key in sorted_vals:\n",
        "    classname = foldername_to_class[key]\n",
        "    categories.append(classname)\n",
        "\n",
        "print(\"Categories\",categories)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68KJ5XYHgXdN",
        "outputId": "5e11700a-d37a-412d-c77f-33643821194e"
      },
      "id": "68KJ5XYHgXdN",
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image to Folder Index {'n01440764': 0, 'n02102040': 1, 'n02979186': 2, 'n03000684': 3, 'n03028079': 4, 'n03394916': 5, 'n03417042': 6, 'n03425413': 7, 'n03445777': 8, 'n03888257': 9}\n",
            "Categories ['tench', 'dog', 'cassette player', 'chain saw', 'church', 'French horn', 'garbage truck', 'gas pump', 'golf ball', 'parachute']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------------------------------------------------------------------\n",
        "# Initialise the data loaders\n",
        "# -------------------------------------------------------------------------------------------------------\n",
        "\n",
        "workers = 2\n",
        "pin_memory = True\n",
        "batch_size = 64\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True, #IMPORTANT otherwise the data is not shuffled\n",
        "    num_workers=workers,\n",
        "    pin_memory=pin_memory,\n",
        "    sampler=None,\n",
        ")\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=workers,\n",
        "    pin_memory=pin_memory,\n",
        ")"
      ],
      "metadata": {
        "id": "5TWhGE5TgXgE"
      },
      "id": "5TWhGE5TgXgE",
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------------------------------------------------------------------\n",
        "#  Test the model - Find accuracy and per class\n",
        "# -------------------------------------------------------------------------------------------------------\n",
        "\n",
        "print(\"Image to Folder Index\",train_dataset.class_to_idx)\n",
        "\n",
        "# move the input and model to GPU for speed if available\n",
        "if torch.cuda.is_available():\n",
        "    model.to(\"cuda\")\n",
        "\n",
        "confusion_matrix = np.zeros((len(categories),len(categories)))\n",
        "\n",
        "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
        "with torch.no_grad():\n",
        "    model.eval() #IMPORTANT set model to eval mode before inference\n",
        "    # correct = 0\n",
        "    # total = 0\n",
        "\n",
        "\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # ------------------------------------------------------------------------------------------\n",
        "        # Predict for the batch of images\n",
        "        # ------------------------------------------------------------------------------------------\n",
        "        outputs = model(images)  #Outputs= torch.Size([64, 10]) Probability of each of the 10 classes\n",
        "        _, predicted = torch.max(outputs.data, 1) # get the class with the highest Probability out Given 1 per image # predicted= torch.Size([64])\n",
        "        # total += labels.size(0) #labels= torch.Size([64])  This is the truth value per image - the right class\n",
        "        # correct += (predicted == labels).float().sum().item()  # Find which are correctly classified\n",
        "        \n",
        "        # Below illustrates the above Torch Tensor semantics\n",
        "        # >>> import torch\n",
        "        # >>> some_integers = torch.tensor((2, 3, 5, 7, 11, 13, 17, 19))\n",
        "        # >>> some_integers3 = torch.tensor((12, 3, 5, 7, 11, 13, 17, 19))\n",
        "        # >>> (some_integers ==some_integers3)*(some_integers == 3)\n",
        "        # tensor([False,  True, False, False, False, False, False, False])\n",
        "        # >>> ((some_integers ==some_integers3)*(some_integers >12)).sum().item()\n",
        "        # 3\n",
        "        \n",
        "        # ------------------------------------------------------------------------------------------\n",
        "        #  Lets check also which classes are wrongly predicted with other classes  to create a MultiClass confusion matrix\n",
        "        # ------------------------------------------------------------------------------------------\n",
        "\n",
        "        mask=(predicted != labels) # Wrongly predicted\n",
        "        wrong_predicted =torch.masked_select(predicted,mask)\n",
        "        wrong_labels =torch.masked_select(labels,mask)\n",
        "        wrongly_zipped = zip(wrong_labels,wrong_predicted)\n",
        "\n",
        "        mask=(predicted == labels) # Rightly predicted\n",
        "        rightly_predicted =torch.masked_select(predicted,mask)\n",
        "        right_labels =rightly_predicted #same torch.masked_select(labels,mask)\n",
        "        rightly_zipped = zip(right_labels,rightly_predicted)\n",
        "        \n",
        "        # Note that this is for a single batch - add to the list associated with class\n",
        "        for _,j in enumerate(wrongly_zipped):\n",
        "            k = j[0].item() # label\n",
        "            l = j[1].item() # predicted\n",
        "            confusion_matrix[k][l] +=1\n",
        "       \n",
        "        # Note that this is for a single batch - add to the list associated with class\n",
        "        for _,j in enumerate(rightly_zipped):\n",
        "            k = j[0].item() # label\n",
        "            l = j[1].item() # predicted\n",
        "            confusion_matrix[k][l] +=1\n",
        "    \n",
        "    #print(\"Confusion Matrix1=\\n\",confusion_matrix)\n",
        "    # ------------------------------------------------------------------------------------------\n",
        "    # Print Confusion matrix in Pretty print format\n",
        "    # ------------------------------------------------------------------------------------------\n",
        "    print(categories)\n",
        "    for i in range(len(categories)):\n",
        "        for j in range(len(categories)):\n",
        "            print(f\"\\t{confusion_matrix[i][j]}\",end='')\n",
        "        print(f\"\\t{categories[i]}\\n\",end='')\n",
        "    # ------------------------------------------------------------------------------------------\n",
        "    # Calculate Accuracy per class\n",
        "    # ------------------------------------------------------------------------------------------\n",
        "    print(\"---------------------------------------\")\n",
        "    print(f\"Accuracy/precision from confusion matrix is {round(confusion_matrix.trace()/confusion_matrix.sum(),2)}\")\n",
        "    print(\"---------------------------------------\")\n",
        "    for i in range(len(categories)):\n",
        "        print(f\"---Accuracy for class {categories[i]} = {round(confusion_matrix[i][i]/confusion_matrix[i].sum(),2)}\")\n",
        "    \n",
        "    # ---------------------------------------------------\n",
        "    # Plot this in a good figure\n",
        "    # ---------------------------------------------------\n",
        "        \n",
        "\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111)\n",
        "    plt.title('Confusion Matrix', fontsize=18)\n",
        "    ax.matshow(confusion_matrix, cmap=plt.cm.Blues, alpha=0.7)\n",
        "    ax.set_xticklabels([''] + categories,rotation=90)\n",
        "    ax.set_yticklabels([''] + categories)\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    for i in range(confusion_matrix.shape[0]):\n",
        "        for j in range(confusion_matrix.shape[1]):\n",
        "            ax.text(x=j, y=i,s=int(confusion_matrix[i, j]), va='center', ha='center', size='xx-small')\n",
        "            if ( i==j):\n",
        "                acc = round(confusion_matrix[i][i]/confusion_matrix[i].sum(),2)\n",
        "                ax.text(x=len(categories)+1, y=i,s=acc, va='center', ha='center', size='xx-small')\n",
        "    plt.savefig(\"confusion_matrix_\"+modelname +\".jpg\")\n",
        "\n",
        "    # correct = 0\n",
        "    # total = 0\n",
        "    # for images, labels in train_loader:\n",
        "    #     images = images.to(device)\n",
        "    #     labels = labels.to(device)\n",
        "    #     outputs = model(images)\n",
        "    #     _, predicted = torch.max(outputs.data, 1)\n",
        "    #     total += labels.size(0)\n",
        "    #     correct += (predicted == labels).float().sum().item()\n",
        "    # # this is not really not needed- but just to cross check if what we calculated during training is accurate\n",
        "    # print(\n",
        "    #     \"Accuracy of the network on the {} Train images: {} %\".format(\n",
        "    #         total, 100 * correct / total\n",
        "    #     )\n",
        "    # )\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 801
        },
        "id": "0B8Lwz2Kg2Kj",
        "outputId": "fd09e0d7-1528-4419-96c7-da4783cfc080"
      },
      "id": "0B8Lwz2Kg2Kj",
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image to Folder Index {'n01440764': 0, 'n02102040': 1, 'n02979186': 2, 'n03000684': 3, 'n03028079': 4, 'n03394916': 5, 'n03417042': 6, 'n03425413': 7, 'n03445777': 8, 'n03888257': 9}\n",
            "['tench', 'dog', 'cassette player', 'chain saw', 'church', 'French horn', 'garbage truck', 'gas pump', 'golf ball', 'parachute']\n",
            "\t315.0\t1.0\t3.0\t1.0\t1.0\t0.0\t1.0\t0.0\t17.0\t48.0\ttench\n",
            "\t71.0\t274.0\t2.0\t0.0\t1.0\t0.0\t0.0\t1.0\t12.0\t34.0\tdog\n",
            "\t48.0\t12.0\t108.0\t1.0\t2.0\t0.0\t7.0\t1.0\t125.0\t53.0\tcassette player\n",
            "\t114.0\t27.0\t5.0\t7.0\t7.0\t0.0\t11.0\t0.0\t95.0\t120.0\tchain saw\n",
            "\t57.0\t5.0\t14.0\t4.0\t240.0\t0.0\t23.0\t0.0\t17.0\t49.0\tchurch\n",
            "\t62.0\t41.0\t9.0\t5.0\t8.0\t0.0\t6.0\t0.0\t31.0\t232.0\tFrench horn\n",
            "\t158.0\t4.0\t22.0\t2.0\t2.0\t0.0\t124.0\t0.0\t30.0\t47.0\tgarbage truck\n",
            "\t60.0\t11.0\t103.0\t0.0\t13.0\t0.0\t64.0\t8.0\t93.0\t67.0\tgas pump\n",
            "\t50.0\t6.0\t13.0\t1.0\t2.0\t0.0\t0.0\t0.0\t260.0\t67.0\tgolf ball\n",
            "\t52.0\t4.0\t0.0\t0.0\t3.0\t0.0\t1.0\t0.0\t33.0\t297.0\tparachute\n",
            "---------------------------------------\n",
            "Accuracy/precision from confusion matrix is 0.42\n",
            "---------------------------------------\n",
            "---Accuracy for class tench = 0.81\n",
            "---Accuracy for class dog = 0.69\n",
            "---Accuracy for class cassette player = 0.3\n",
            "---Accuracy for class chain saw = 0.02\n",
            "---Accuracy for class church = 0.59\n",
            "---Accuracy for class French horn = 0.0\n",
            "---Accuracy for class garbage truck = 0.32\n",
            "---Accuracy for class gas pump = 0.02\n",
            "---Accuracy for class golf ball = 0.65\n",
            "---Accuracy for class parachute = 0.76\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Model Explain-ability GradCam***\n",
        "Gradcam helps one visualize which parts of the images are important for the CNN when it classifies an object with high probability. After testing a model, you can use this to visualize and debug the test results\n",
        "\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\n",
        "\n",
        "Using Grad CAM libray to visualize the gradients of the last layer\n",
        "and to see if the model has learned the features of the images"
      ],
      "metadata": {
        "id": "VBku6avje6oR"
      },
      "id": "VBku6avje6oR"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install grad_cam"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjtv9AdHsBA1",
        "outputId": "13a37e3a-2369-4731-cea8-f4c22af0c3b1"
      },
      "id": "kjtv9AdHsBA1",
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting grad_cam\n",
            "  Downloading grad-cam-1.4.6.tar.gz (7.8 MB)\n",
            "\u001b[K     || 7.8 MB 5.0 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from grad_cam) (1.0.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.8/dist-packages (from grad_cam) (7.1.2)\n",
            "Requirement already satisfied: torch>=1.7.1 in /usr/local/lib/python3.8/dist-packages (from grad_cam) (1.13.0+cu116)\n",
            "Collecting ttach\n",
            "  Downloading ttach-0.0.3-py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from grad_cam) (1.21.6)\n",
            "Requirement already satisfied: torchvision>=0.8.2 in /usr/local/lib/python3.8/dist-packages (from grad_cam) (0.14.0+cu116)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from grad_cam) (3.2.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.8/dist-packages (from grad_cam) (4.6.0.66)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from grad_cam) (4.64.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.7.1->grad_cam) (4.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision>=0.8.2->grad_cam) (2.23.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->grad_cam) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->grad_cam) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->grad_cam) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->grad_cam) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib->grad_cam) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision>=0.8.2->grad_cam) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision>=0.8.2->grad_cam) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision>=0.8.2->grad_cam) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision>=0.8.2->grad_cam) (1.24.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->grad_cam) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->grad_cam) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->grad_cam) (1.7.3)\n",
            "Building wheels for collected packages: grad-cam\n",
            "  Building wheel for grad-cam (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for grad-cam: filename=grad_cam-1.4.6-py3-none-any.whl size=38261 sha256=af00c3c86da9b15586876eab3f175c2ef4916b39e500ed629c9d450d849ae52d\n",
            "  Stored in directory: /root/.cache/pip/wheels/53/02/43/1f75726b5c28459596067ad91e36951463c01273eef661f09f\n",
            "Successfully built grad-cam\n",
            "Installing collected packages: ttach, grad-cam\n",
            "Successfully installed grad-cam-1.4.6 ttach-0.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pytorch_grad_cam import (\n",
        "    GradCAM,\n",
        "    HiResCAM,\n",
        "    ScoreCAM,\n",
        "    GradCAMPlusPlus,\n",
        "    AblationCAM,\n",
        "    XGradCAM,\n",
        "    EigenCAM,\n",
        "    FullGrad,\n",
        ")\n",
        "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
        "\n",
        "\n",
        "from importlib.resources import path\n",
        "from PIL import Image\n",
        "from torchvision import transforms, datasets\n",
        "import torch\n",
        "#from models import resnet, alexnet, mycnn, mycnn2\n",
        "import numpy as np\n",
        "import os\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "I-xOowrHeu1z"
      },
      "id": "I-xOowrHeu1z",
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#-----------------------------------------------------------------------------------------------------\n",
        "# Order the categories as per how Dataloader loads it\n",
        "#-----------------------------------------------------------------------------------------------------\n",
        "#/content/drive/MyDrive/Model Data/imagenette/imagenette2-320\n",
        "\n",
        "data_dir = \"../imagenette/imagenette2-320\"\n",
        "train_dir = os.path.join(data_dir, \"train\")\n",
        "train_dataset = datasets.ImageFolder(train_dir,[])\n",
        "\n",
        "foldername_to_class = { 'n02102040' : \"dog\",\n",
        "                        'n01440764': \"tench\",\n",
        "                        'n02979186': \"cassette player\", \n",
        "                        'n03000684': \"chain saw\",\n",
        "                        'n03028079': \"church\",\n",
        "                        'n03394916': \"French horn\",\n",
        "                        'n03417042': \"garbage truck\",\n",
        "                        'n03425413': \"gas pump\",\n",
        "                        'n03445777':  \"golf ball\",\n",
        "                        'n03888257': \"parachute\" }\n",
        "\n",
        "\n",
        "# sort as value to fit the directory order to labels to be sure\n",
        "print(\"Image to Folder Index\",train_dataset.class_to_idx)\n",
        "sorted_vals = dict(sorted(train_dataset.class_to_idx.items(), key=lambda item: item[1]))\n",
        "categories =[]\n",
        "for key in sorted_vals:\n",
        "    classname = foldername_to_class[key]\n",
        "    categories.append(classname)\n",
        "\n",
        "print(\"Categories\",categories)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8F9z65E4eu50",
        "outputId": "de037d09-ffe3-407c-a1c3-88930512a961"
      },
      "id": "8F9z65E4eu50",
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image to Folder Index {'n01440764': 0, 'n02102040': 1, 'n02979186': 2, 'n03000684': 3, 'n03028079': 4, 'n03394916': 5, 'n03417042': 6, 'n03425413': 7, 'n03445777': 8, 'n03888257': 9}\n",
            "Categories ['tench', 'dog', 'cassette player', 'chain saw', 'church', 'French horn', 'garbage truck', 'gas pump', 'golf ball', 'parachute']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose a saved Model - comment out the rest\n",
        "# I commented this:\n",
        "#modelname = \"resnet50\"\n",
        "\n",
        "# Choose a saved Model - assign the name you want to test with\n",
        "# (assuming that you have trained the models)\n",
        "#modelname = \"resnet50\"\n",
        "#resize_size =(1,1)\n",
        "#if modelname == \"mycnn\":\n",
        "#    resize_size = (227, 227)\n",
        "#    model = mycnn.MyCNN()\n",
        "#    path = \"mycnn_18:07_October142022.pth\"\n",
        "#    path =\"mycnn_13:01_October272022.pth\" #dual class only\n",
        "#    resize_to = transforms.Resize(resize_size)\n",
        "#if modelname == \"mycnn2\":\n",
        "#    resize_size = (227, 227)\n",
        "#    model = mycnn2.MyCNN2()\n",
        "#    path =\"mycnn2_16:43_October182022.pth\"\n",
        "#    resize_to = transforms.Resize(resize_size)\n",
        "#if modelname == \"alexnet\":\n",
        "#    resize_size = (227,227)\n",
        "#    model = alexnet.AlexNet()\n",
        "#    path = \"alexnet_20:56_October102022.pth\"\n",
        "#    resize_to = transforms.Resize(resize_size)\n",
        "#if modelname == \"resnet50\":\n",
        "#    model = resnet.ResNet50(img_channel=3, num_classes=10)\n",
        "#    resize_size =(227,227)\n",
        "    #path = \"./RestNet50_12:26_August082022.pth\" # without augumentation\n",
        "#    path = \"RestNet50_13:49_September102022.pth\" #with augumentation\n",
        "#    path = \"RestNet50_16:54_October062022.pth\" #with cartoon dogs\n",
        "#    path = \"RestNet50_11:43_October072022.pth\"   # trained with more dog images from imagenet\n",
        "#    path =\"RestNet50_11:45_November072022.pth\" #227*227\n",
        "   # resize_to = transforms.Resize(227,227)\n",
        "\n",
        "#path = \"cnn/saved_models/\" +path\n",
        "#model.load_state_dict(torch.load(path))\n",
        "model.eval()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1IJO6GV7iswO",
        "outputId": "7653037f-61ff-4d17-f23a-c2351d602962"
      },
      "id": "1IJO6GV7iswO",
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (4): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (5): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def flatten_model(modules):\n",
        "    def flatten_list(_2d_list):\n",
        "        flat_list = []\n",
        "        # Iterate through the outer list\n",
        "        for element in _2d_list:\n",
        "            if type(element) is list:\n",
        "                # If the element is of type list, iterate through the sublist\n",
        "                for item in element:\n",
        "                    flat_list.append(item)\n",
        "            else:\n",
        "                flat_list.append(element)\n",
        "        return flat_list\n",
        "\n",
        "    ret = []\n",
        "    try:\n",
        "        for _, n in modules:\n",
        "            ret.append(loopthrough(n))\n",
        "    except:\n",
        "        try:\n",
        "            if str(modules._modules.items()) == \"odict_items([])\":\n",
        "                ret.append(modules)\n",
        "            else:\n",
        "                for _, n in modules._modules.items():\n",
        "                    ret.append(loopthrough(n))\n",
        "        except:\n",
        "            ret.append(modules)\n",
        "    return flatten_list(ret)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uH-FOTGvjDpN"
      },
      "id": "uH-FOTGvjDpN",
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the target for the CAM layer; Add all the layers in the model\n",
        "target_layers =[]\n",
        "module_list =[module for module in model.modules()]\n",
        "\n",
        "\n",
        "flatted_list= flatten_model(module_list)\n",
        "\n",
        "\n",
        "print(\"------------------------\")\n",
        "for count, value in enumerate(flatted_list):\n",
        "    \n",
        "    if isinstance(value, (nn.Conv2d,nn.AvgPool2d,nn.BatchNorm2d)):\n",
        "    #if isinstance(value, (nn.Conv2d)):\n",
        "        print(count, value)\n",
        "        target_layers.append(value)\n",
        "\n",
        "print(\"------------------------\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYelsp78jIZF",
        "outputId": "d821603f-992b-48de-afec-7c31616eb4ae"
      },
      "id": "vYelsp78jIZF",
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------\n",
            "1 Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "2 BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "7 Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "8 BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "9 Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "10 BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "11 Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "12 BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "15 Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "16 BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "18 Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "19 BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "20 Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "21 BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "22 Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "23 BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "26 Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "27 BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "28 Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "29 BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "30 Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "31 BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "35 Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "36 BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "37 Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "38 BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "39 Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "40 BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "43 Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "44 BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "46 Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "47 BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "48 Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "49 BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "50 Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "51 BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "54 Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "55 BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "56 Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "57 BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "58 Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "59 BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "62 Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "63 BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "64 Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "65 BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "66 Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "67 BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "71 Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "72 BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "73 Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "74 BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "75 Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "76 BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "79 Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "80 BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "82 Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "83 BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "84 Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "85 BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "86 Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "87 BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "90 Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "91 BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "92 Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "93 BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "94 Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "95 BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "98 Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "99 BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "100 Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "101 BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "102 Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "103 BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "106 Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "107 BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "108 Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "109 BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "110 Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "111 BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "114 Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "115 BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "116 Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "117 BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "118 Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "119 BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "123 Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "124 BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "125 Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "126 BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "127 Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "128 BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "131 Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "132 BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "134 Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "135 BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "136 Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "137 BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "138 Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "139 BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "142 Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "143 BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "144 Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "145 BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "146 Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "147 BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Alternative is to add specific layers and check\n",
        "# if modelname=='resnet50':\n",
        "#     #target_layers = [module_list[142],module_list[143],module_list[144],module_list[145],module_list[146],module_list[147]]\n",
        "#     target_layers = [module_list[35],module_list[36],module_list[37],module_list[38],module_list[39],module_list[40]]\n",
        "# if modelname=='mycnn':\n",
        "#     target_layers = [module_list[11],module_list[8],module_list[5],module_list[2],module_list[4],module_list[7],module_list[10],module_list[13]] # CNN and Avg pooling\n",
        "#     #target_layers = [module_list[11],module_list[8],module_list[5],module_list[2]] # CNN only\n",
        "\n",
        "    \n",
        "# Construct the CAM object once, and then re-use it on many images:\n",
        "cam = GradCAM(model=model, target_layers=target_layers, use_cuda=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "fSpy9mwkjSKU"
      },
      "id": "fSpy9mwkjSKU",
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the images\n",
        "#/content/drive/MyDrive/Model Data/test-images\n",
        "\n",
        "resize_size =(227,227)\n",
        "\n",
        "test_images = ['test-tench.jpg','fish_boy.jpg','test-church.jpg','test-garbagetruck.jpg','test-truck.jpg','test-dog.jpg','train_dog.png',\n",
        "\"test-englishspringer.jpg\",\"test_dogcartoon.jpg\",\"test_chaingsaw.jpg\",\"test_chainsawtrain.jpg\",\"test_frenchhorn.jpg\",\n",
        "\"test_frenchhorntrain.jpg\",\"test-golfball.jpg\"]\n",
        "\n",
        "for filename in test_images:\n",
        "    input_image = Image.open('../test-images/'+filename)\n",
        "\n",
        "    preprocess = transforms.Compose(\n",
        "        [\n",
        "            resize_to,\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(\n",
        "                mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
        "            ),  # IMPORTANT: normalize for pre-trained models\n",
        "        ]\n",
        "    )\n",
        "    input_tensor = preprocess(input_image)\n",
        "    print(\"Input Tensor Shape:\", input_tensor.shape)\n",
        "    input_batch = input_tensor.unsqueeze(0)  # create a mini-batch as expected by the model\n",
        "    # move the input and model to GPU for speed if available\n",
        "    if torch.cuda.is_available():\n",
        "        input_batch = input_batch.to(\"cuda\")\n",
        "        model.to(\"cuda\")\n",
        "        input_tensor = input_tensor.to(\"cuda\")\n",
        "\n",
        "    # We have to specify the target we want to generate\n",
        "    # the Class Activation Maps for.\n",
        "    # If targets is None, the highest scoring category\n",
        "    # will be used for every image in the batch.\n",
        "    # Here we use ClassifierOutputTarget, but you can define your own custom targets\n",
        "\n",
        "    # That are, for example, combinations of categories, or specific outputs in a non standard model.\n",
        "    targets = [ClassifierOutputTarget(6)] #0 for finch ?\n",
        "\n",
        "    # You can also pass aug_smooth=True and eigen_smooth=True, to apply smoothing.\n",
        "    grayscale_cam = cam(input_batch, targets=None,aug_smooth=True)\n",
        "    #print( \"len grayscale_cam\",len(grayscale_cam),grayscale_cam.shape)\n",
        "\n",
        "    # In this example grayscale_cam has only one image in the batch:\n",
        "    grayscale_cam = grayscale_cam[0, :]\n",
        "    # from PIL import Image\n",
        "    # im = Image.fromarray(grayscale_cam)\n",
        "    # if im.mode != 'RGB':\n",
        "    #     im = im.convert('L')\n",
        "    # im.save(\"grayscale_cam.jpeg\"\n",
        "\n",
        "#/content/drive/MyDrive/Model Data/gradcam_out\n",
        "\n",
        "    img=np.array(input_image.resize(resize_size),np.float32)\n",
        "    img = img.reshape(img.shape[1],img.shape[0],img.shape[2])\n",
        "    #print(\"img shape\",img.shape,img.max())\n",
        "    img = img/255\n",
        "    visualization = show_cam_on_image(img, grayscale_cam, use_rgb=True)\n",
        "    #cam_images = [show_cam_on_image(img, grayscale, use_rgb=True) for img, grayscale in zip(input_image, grayscale_cam)]\n",
        "    visualization = Image.fromarray(visualization)\n",
        "    out_file_name =\"../gradcam_out/\" +modelname+ \"_\" + os.path.basename(filename)\n",
        "    visualization.save(out_file_name)\n",
        "    #print(\"Visualization saved- now trying to show (GUI mode)\")\n",
        "    im = Image.open(out_file_name)\n",
        "    im.show()\n",
        "    \n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cKrczX2KjVl2",
        "outputId": "283af123-411b-4eca-ad28-9fc7b2e90001"
      },
      "id": "cKrczX2KjVl2",
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Tensor Shape: torch.Size([3, 227, 227])\n",
            "Input Tensor Shape: torch.Size([3, 227, 227])\n",
            "Input Tensor Shape: torch.Size([3, 227, 227])\n",
            "Input Tensor Shape: torch.Size([3, 227, 227])\n",
            "Input Tensor Shape: torch.Size([3, 227, 227])\n",
            "Input Tensor Shape: torch.Size([3, 227, 227])\n",
            "Input Tensor Shape: torch.Size([3, 227, 227])\n",
            "Input Tensor Shape: torch.Size([3, 227, 227])\n",
            "Input Tensor Shape: torch.Size([3, 227, 227])\n",
            "Input Tensor Shape: torch.Size([3, 227, 227])\n",
            "Input Tensor Shape: torch.Size([3, 227, 227])\n",
            "Input Tensor Shape: torch.Size([3, 227, 227])\n",
            "Input Tensor Shape: torch.Size([3, 227, 227])\n",
            "Input Tensor Shape: torch.Size([3, 227, 227])\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}