{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maryamdarei/CNN/blob/main/New_Final_CNN_with_GradCAM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "6a71ad48",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6a71ad48",
        "outputId": "77a447f5-bb3b-40d1-852e-422d3288cdf1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 7.8 MB 5.0 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for grad-cam (PEP 517) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "pip install -q grad-cam==1.4.3\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install torch==1.8.1 torchvision==0.9.1 -f https://download.pytorch.org/whl/cu101/torch_stable.html"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 536
        },
        "id": "iNoMNhwUCWOV",
        "outputId": "1c02864f-1235-417d-e7d8-bd53bd132ea2"
      },
      "id": "iNoMNhwUCWOV",
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://download.pytorch.org/whl/cu101/torch_stable.html\n",
            "Collecting torch==1.8.1\n",
            "  Using cached https://download.pytorch.org/whl/cu101/torch-1.8.1%2Bcu101-cp38-cp38-linux_x86_64.whl (763.7 MB)\n",
            "Collecting torchvision==0.9.1\n",
            "  Downloading https://download.pytorch.org/whl/cu101/torchvision-0.9.1%2Bcu101-cp38-cp38-linux_x86_64.whl (17.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 17.3 MB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torch==1.8.1) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch==1.8.1) (4.4.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.8/dist-packages (from torchvision==0.9.1) (7.1.2)\n",
            "Installing collected packages: torch, torchvision\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.4.0\n",
            "    Uninstalling torch-1.4.0:\n",
            "      Successfully uninstalled torch-1.4.0\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.5.0\n",
            "    Uninstalling torchvision-0.5.0:\n",
            "      Successfully uninstalled torchvision-0.5.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.14.0 requires torch==1.13.0, but you have torch 1.8.1+cu101 which is incompatible.\n",
            "torchaudio 0.13.0+cu116 requires torch==1.13.0, but you have torch 1.8.1+cu101 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.8.1+cu101 torchvision-0.9.1+cu101\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch",
                  "torchvision"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "md-zd0rK30dJ",
        "outputId": "0ababd85-02a6-41e5-bd32-7097de50e6df"
      },
      "id": "md-zd0rK30dJ",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5dfc46a7",
      "metadata": {
        "id": "5dfc46a7"
      },
      "source": [
        "### Utility functions\n",
        "I like to write my own code, so I have prepared some helper functions to easily train and evaluate the model.\n",
        "\n",
        "I will be using CrossEntropyLoss in PyTorch, so the model will be trained by using Cross Entropy loss, the implementation already applies logsoftmax to the outputs. So we have to manually apply softmax to model's outputs while evaluating and preparing the predictions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "ca7c0568",
      "metadata": {
        "id": "ca7c0568"
      },
      "outputs": [],
      "source": [
        "# utilization\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from glob import glob\n",
        "\n",
        "# data manipulation and visualization tools\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# cross-validaion and evaluation tools\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "\n",
        "# model development and data preparation\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.utils.data import SubsetRandomSampler, ConcatDataset\n",
        "from torch.utils.data import random_split\n",
        "from torchvision.io import read_image\n",
        "from torchvision import transforms as t\n",
        "\n",
        "\n",
        "# GradCAM implementations and some utility tools\n",
        "from pytorch_grad_cam import GradCAM, HiResCAM, ScoreCAM, GradCAMPlusPlus, AblationCAM, XGradCAM, EigenCAM, FullGrad, LayerCAM, EigenGradCAM\n",
        "from pytorch_grad_cam.utils.model_targets import ClassifierOutputSoftmaxTarget\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1db7c822",
      "metadata": {
        "id": "1db7c822"
      },
      "source": [
        "### GradCAM\n",
        "\n",
        "GradCAM stands for Gradient-weighted class activation mappings. In short, we will weight the layer activations by gradients, which will generate a heatmap. Then, we can visualize the parts of the image that has the most impact on the model's outputs. Weighting the layer activations by calculated gradients is not enough though, we have lots of channels in a convolutional layer so we will average the gradients.\n",
        "\n",
        "I will be using this package to use GradCAM on our networks. The package has different GradCAM implementations, we will generally be using the standard one but I'm including the descriptions of a few of them:\n",
        "\n",
        "   ##### 1- GradCAM: Weight the 2D activations by the average gradient\n",
        "\n",
        "   ##### 2- HiResCAM: Like GradCAM but element-wise multiply the activations with the gradients; provably guaranteed faithfulness for certain models\n",
        "\n",
        "   ##### 3- GradCAMElementWise: Like GradCAM but element-wise multiply the activations with the gradients then apply a ReLU operation before summing\n",
        "\n",
        "   ##### 4- GradCAM++: Like GradCAM but uses second order gradients\n",
        "\n",
        "   ##### 5- XGradCAM: Like GradCAM but scale the gradients by the normalized activations\n",
        "\n",
        "   ##### 6- AblationCAM: Zero out activations and measure how the output drops (this package includes a fast batched implementation)\n",
        "\n",
        "   ##### 7- ScoreCAM: Perbutate the image by the scaled activations and measure how the output drops\n",
        "\n",
        "   ##### 8- EigenCAM: Takes the first principle component of the 2D Activations (no class discrimination, but seems to give great results)\n",
        "\n",
        "   ##### 9- EigenGradCAM: Like EigenCAM but with class discrimination: First principle component of Activations*Grad. Looks like GradCAM, but cleaner\n",
        "\n",
        "   ##### 10- LayerCAM: Spatially weight the activations by positive gradients. Works better, especially in lower layers\n",
        "\n",
        "   ##### 11- FullGrad: Computes the gradients of the biases from all over the network, and then sums them\n",
        "\n",
        "   ##### 12- Deep Feature Factorizations: Non Negative Matrix Factorization on the 2D activations\n",
        "\n",
        "The descriptions were taken from the github repo\n",
        "\n",
        "A little helper included below to calculate and plot the GradCAM results:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "8d0e829f",
      "metadata": {
        "id": "8d0e829f"
      },
      "outputs": [],
      "source": [
        "# CNN Model With PyTorch For Image Classification\n",
        "# Load in relevant libraries, and alias where appropriate\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import sys\n",
        "import os\n",
        "import glob\n",
        "import cv2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "b737ead3",
      "metadata": {
        "id": "b737ead3"
      },
      "outputs": [],
      "source": [
        "def random_split_ratio(dataset, test_size=.2, random_state=None):\n",
        "    L = len(dataset)\n",
        "    n_second = int(L*test_size)\n",
        "    n_first = L - n_second\n",
        "    if random_state:\n",
        "        first_split, second_split = random_split(dataset, lengths=[n_first, n_second], generator=torch.Generator().manual_seed(random_state))\n",
        "    else:\n",
        "        first_split, second_split = random_split(dataset, lengths=[n_first, n_second])\n",
        "\n",
        "    return first_split, second_split\n",
        "\n",
        "\n",
        "dataset = \"/content/drive/MyDrive/Herbarium Data-2019/small-train-AUG\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "3d987a4a",
      "metadata": {
        "id": "3d987a4a"
      },
      "outputs": [],
      "source": [
        "def show(dataset, N=5, labels=None, figsize=(20, 20)):\n",
        "    \"\"\" Shows random N samples from the dataset \"\"\"\n",
        "    idxs = np.random.randint(0, len(dataset)-1, N)\n",
        "\n",
        "    fig, axs = plt.subplots(ncols=len(idxs), squeeze=False, figsize=figsize)\n",
        "\n",
        "    for i, idx in enumerate(idxs):\n",
        "        sample = dataset[idx]\n",
        "        \n",
        "        if isinstance(sample, tuple): # then it is in the form (x, y)\n",
        "            sample, label = sample\n",
        "            label = int(label.item())\n",
        "            if labels:\n",
        "                label = labels[label]\n",
        "            axs[0, i].title.set_text(label)\n",
        "\n",
        "        sample = sample.permute(1, 2, 0)\n",
        "\n",
        "        axs[0, i].imshow(sample, cmap='gray')\n",
        "        axs[0, i].set(xticklabels = [], yticklabels = [], xticks = [], yticks = [])\n",
        "\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "0c280377",
      "metadata": {
        "id": "0c280377"
      },
      "outputs": [],
      "source": [
        "\n",
        "def evaluate(dataset, model, device='cpu', **dataloader_args):\n",
        "    dataloader = DataLoader(dataset, **dataloader_args)\n",
        "    preds = []\n",
        "    labels = []\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        model.to(device)\n",
        "        \n",
        "        for x_batch, y_batch in tqdm(dataloader):\n",
        "            x_batch = x_batch.to(device)\n",
        "            y_batch = y_batch.tolist()\n",
        "            \n",
        "            outs = model(x_batch).detach().cpu()\n",
        "            predictions = torch.argmax(torch.softmax(outs, 1), 1).tolist()\n",
        "            \n",
        "            # extend the `preds` and `labels` lists with predictions and true labels\n",
        "            preds.extend(predictions)\n",
        "            labels.extend(y_batch)\n",
        "            \n",
        "    report = classification_report(labels, preds, digits = 3)\n",
        "    print(report)\n",
        "    return report\n",
        "\n",
        "def train_cv(model_, loss_fn_, optimizer_, dataset, batch_size=256, device='cpu', cv=5, epochs=10, random_state=None, **opt_args):\n",
        "    training_losses = []\n",
        "    validation_losses = [] \n",
        "    fold_models = []\n",
        "    \n",
        "    # define a pseudo number generator and seed it manually if random_state is given\n",
        "    if random_state:\n",
        "        gen = torch.Generator().manual_seed(random_state)\n",
        "    else:\n",
        "        gen = None\n",
        "        \n",
        "    kfold = KFold(n_splits=cv, random_state=random_state, shuffle=True)\n",
        "    \n",
        "    for fold, (train_idx, val_idx) in enumerate(kfold.split(np.arange(len(dataset)))):\n",
        "        fold_training_losses = []\n",
        "        fold_validation_losses = []\n",
        "        \n",
        "        print(\"CURRENT FOLD:\", fold+1)\n",
        "        \n",
        "        train_sampler = SubsetRandomSampler(train_idx, generator=gen)\n",
        "        validation_sampler = SubsetRandomSampler(val_idx, generator=gen)\n",
        "        \n",
        "        train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n",
        "        val_loader = DataLoader(dataset, batch_size=batch_size, sampler=validation_sampler)\n",
        "        \n",
        "        # train the model using training and validation sets\n",
        "        fold_training_losses, fold_validation_losses = train(model_, loss_fn_, optimizer_,\n",
        "                                                             train_loader=train_loader, val_loader=val_loader,\n",
        "                                                             device=device, epochs=epochs, **opt_args)\n",
        "\n",
        "        # save losses for the fold in respective arrays \n",
        "        training_losses.append(fold_training_losses)\n",
        "        validation_losses.append(fold_validation_losses)\n",
        "        \n",
        "    print(\"----Cross validation finished----\")\n",
        "    print(\"Average losses:\")\n",
        "    for fold_idx, (training_loss, val_loss) in enumerate(zip(training_losses, validation_losses)):\n",
        "        avg_training_loss = sum(training_loss)/epochs\n",
        "        avg_val_loss = sum(val_loss)/epochs\n",
        "        print(f\"FOLD: {fold_idx+1} | avg. training loss: {avg_training_loss:.3f} | avg. validation loss: {avg_val_loss:.3f}\")\n",
        "    \n",
        "    else:\n",
        "        return training_losses, validation_losses\n",
        "\n",
        "\n",
        "def train(model_, loss_fn_, optimizer_, \n",
        "          train_loader, val_loader,\n",
        "          return_model=False, device='cpu', \n",
        "          epochs=10, **opt_args):\n",
        "    \n",
        "    \n",
        "    if train_loader.sampler:\n",
        "        n_train = len(train_loader.sampler)\n",
        "        n_val = len(val_loader.sampler)\n",
        "    else:\n",
        "        n_train = len(train_loader.dataset)\n",
        "        n_val = len(val_loader.dataset)\n",
        "    \n",
        "    training_losses = []\n",
        "    validation_losses = [] \n",
        "    \n",
        "    # initialize the model, optimizer and loss function\n",
        "    model = model_()\n",
        "    loss_fn = loss_fn_()\n",
        "    optimizer = optimizer_(model.parameters(), **opt_args)\n",
        "\n",
        "    # pass the model to the given device\n",
        "    model.to(device)\n",
        "\n",
        "    print(\"Number of samples\")\n",
        "    print(\"Training:\", n_train)\n",
        "    print('Validation:', n_val)\n",
        "    for epoch in range(epochs):\n",
        "        # define running losses\n",
        "        epoch_training_running_loss = 0\n",
        "        epoch_val_running_loss = 0\n",
        "\n",
        "        # loop through every batch in the training loader\n",
        "        for x_batch, y_batch in tqdm(train_loader): \n",
        "            # pass the batches to given device\n",
        "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "\n",
        "            # set the gradients to 0 beforehand\n",
        "            # it can also be written after `optimizer.step()`, just a preference.\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "            # get the model outputs and calculate the loss\n",
        "            outs = model(x_batch)\n",
        "            loss = loss_fn(outs, y_batch)\n",
        "\n",
        "            # calculate the gradients and apply an optimization step\n",
        "            loss.backward() \n",
        "            optimizer.step()\n",
        "\n",
        "            # we can use `.item()` method to read the loss value\n",
        "            # since the loss function automatically calculates the loss by averaging the input size,\n",
        "            # we will multiply it with the batch size to add it\n",
        "            # then we can average it by the whole dataset size\n",
        "            # note: it is also possible to average the loss by the number of batches at the end of the epoch (without multiplying with x_batch.size(0))\n",
        "            # but this approach is more understandable I think.\n",
        "            epoch_training_running_loss += (loss.item() * x_batch.size(0))\n",
        "\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            for x_batch, y_batch in tqdm(val_loader):\n",
        "                x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "\n",
        "                outs = model(x_batch)\n",
        "                loss = loss_fn(outs, y_batch)\n",
        "\n",
        "                epoch_val_running_loss += (loss.item() * x_batch.size(0))\n",
        "            model.train()\n",
        "\n",
        "        average_training_loss = epoch_training_running_loss / n_train\n",
        "        average_validation_loss = epoch_val_running_loss / n_val\n",
        "\n",
        "        training_losses.append(average_training_loss)\n",
        "        validation_losses.append(average_validation_loss)\n",
        "        \n",
        "        print(f\"epoch {epoch+1}/{epochs} | avg. training loss: {average_training_loss:.3f}, avg. validation loss: {average_validation_loss:.3f}\")\n",
        "\n",
        "    # return the training and validtion losses, also return the model if return_model is True\n",
        "    if return_model:\n",
        "        return training_losses, validation_losses, model\n",
        "    else:\n",
        "        return training_losses, validation_losses\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "9e50980c",
      "metadata": {
        "id": "9e50980c"
      },
      "outputs": [],
      "source": [
        "\n",
        "def gradcam(model, gradcam_obj, layers, targets, dataset, N=5, use_cuda=False, show_labels=False, idx_to_label=None, **gradcam_params):\n",
        "    random_indices = np.random.randint(0, len(dataset), N)\n",
        "    samples = [dataset[idx][0].unsqueeze(0) for idx in random_indices]\n",
        "    input_tensor = torch.cat(samples, dim=0)\n",
        "    \n",
        "    if show_labels:\n",
        "        labels = [dataset[idx][1].item() for idx in random_indices]\n",
        "        if idx_to_label:\n",
        "            labels = [idx_to_label[label] for label in labels]\n",
        "    \n",
        "    for idx, layer in enumerate(layers):\n",
        "        target_layers = [layer]\n",
        "        \n",
        "        # Construct the CAM object once, and then re-use it on many images.\n",
        "        # since we will be using it for every single convolutional layer, I will redefine it every time\n",
        "        cam = gradcam_obj(model=model, target_layers=target_layers, use_cuda=use_cuda)\n",
        "        \n",
        "        # You can also pass aug_smooth=True and eigen_smooth=True to apply smoothing.\n",
        "        grayscale_cam = cam(input_tensor=input_tensor, targets=targets, **gradcam_params)\n",
        "\n",
        "        images = [input_tensor[idx].permute(1,2,0).numpy() for idx in range(N)]\n",
        "        grayscaled_cam = [grayscale_cam[idx,:] for idx in range(N)]\n",
        "        heatmaps_on_inputs = [show_cam_on_image(img, cam) for img,cam in zip(images, grayscaled_cam)]\n",
        "        \n",
        "        viz_img_list = [images, grayscaled_cam, heatmaps_on_inputs]\n",
        "        subfig_titles = [\"Input Images\", \"Grayscaled Heatmap\", \"Heatmaps on the Inputs\"]\n",
        "        \n",
        "        fig = plt.figure(figsize=(20, 10))\n",
        "        subfigs = fig.subplots(nrows=3, ncols=1) #I change fig.subfigures to fig.subplots\n",
        "\n",
        "        fig.suptitle(f'GradCAM for layer: {idx+1}', fontsize=18, y=1.05)\n",
        "        for subfig_idx, subfig in enumerate(subfigs):\n",
        "            subfig.suptitle(subfig_titles[subfig_idx], y=1)\n",
        "            \n",
        "            viz_list = viz_img_list[subfig_idx]\n",
        "            \n",
        "            axs = subfig.subplots(nrows=1, ncols=N)\n",
        "            for idx in range(N):\n",
        "                axs[idx].imshow(viz_list[idx], cmap='gray')\n",
        "                \n",
        "                if show_labels:\n",
        "                    axs[idx].set_title(labels[idx])\n",
        "                \n",
        "                axs[idx].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
        "            \n",
        "        plt.show()\n",
        "\n",
        "        print('-'*150)\n",
        "        print(\"\\n\\n\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5331cd6",
      "metadata": {
        "id": "a5331cd6",
        "outputId": "515f4595-6529-4630-8cb0-108c6fdb88d9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Figure size 1440x720 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "fig = plt.figure(figsize=(20, 10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "183806f1",
      "metadata": {
        "id": "183806f1",
        "outputId": "07dad669-d982-44f2-f55b-8a187eced802"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([<AxesSubplot:>, <AxesSubplot:>, <AxesSubplot:>], dtype=object)"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "fig.subplots(nrows=3, ncols=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "e482c603",
      "metadata": {
        "id": "e482c603"
      },
      "outputs": [],
      "source": [
        "# Transform Data:\n",
        "# Define relevant variables for the ML task\n",
        "batch_size = 64\n",
        "num_classes = 683\n",
        "learning_rate = 0.001\n",
        "num_epochs = 20\n",
        "\n",
        "# Device will determine whether to run the training on GPU or CPU.\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "cd5fa055",
      "metadata": {
        "id": "cd5fa055"
      },
      "outputs": [],
      "source": [
        "#train and test data directory\n",
        "data_dir = \"/content/drive/MyDrive/Herbarium Data-2019/small-train-AUG\"\n",
        "test_data_dir = \"/content/drive/MyDrive/Herbarium Data-2019/small-validation\"\n",
        "\n",
        "\n",
        "\n",
        "#load the train and test data\n",
        "train_dataset = ImageFolder(data_dir,transform = transforms.Compose([\n",
        "    transforms.Resize((256,256)),transforms.ToTensor()\n",
        "]))\n",
        "\n",
        "val_dataset = ImageFolder(test_data_dir,transforms.Compose([transforms.Resize((256,256)),transforms.ToTensor()]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "c10a79d8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c10a79d8",
        "outputId": "cf2e1839-681c-4179-94c9-a98e7dd039e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 256, 256]) 0\n"
          ]
        }
      ],
      "source": [
        "img, label = train_dataset[0]\n",
        "print(img.shape,label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "00140792",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00140792",
        "outputId": "1e29e594-19a6-429d-f61b-c26742ffe0f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of Train Data : 34225\n",
            "Length of Validation Data : 2679\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data.dataloader import DataLoader\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "batch_size = 128\n",
        "val_size = len(val_dataset)\n",
        "train_size = len(train_dataset) - val_size \n",
        "\n",
        "train_data,val_data = random_split(train_dataset,[train_size,val_size])\n",
        "print(f\"Length of Train Data : {len(train_dataset)}\")\n",
        "print(f\"Length of Validation Data : {len(val_data)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "e58aa448",
      "metadata": {
        "id": "e58aa448"
      },
      "outputs": [],
      "source": [
        "#load the train and validation into batches.\n",
        "train_loader = DataLoader(train_data, batch_size, shuffle = True, num_workers = 4, pin_memory = True)\n",
        "val_loader = DataLoader(val_data, batch_size*2, num_workers = 4, pin_memory = True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "7fb74817",
      "metadata": {
        "id": "7fb74817"
      },
      "outputs": [],
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.cnn_block1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=2, padding=0, dilation=1, groups=1, bias=False), # output shape = (batch, 32, 13, 13)\n",
        "            nn.BatchNorm2d(num_features=32),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.cnn_block2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=0, dilation=1, groups=1, bias=False), # output shape = (batch, 64, 11, 11)\n",
        "            nn.BatchNorm2d(num_features=64),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.cnn_block3 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=0, dilation=1, groups=1, bias=False), # output shape = (batch, 128, 9, 9)\n",
        "            nn.BatchNorm2d(num_features=128),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        \n",
        "        self.fc_layer = nn.Linear(1936512, 10)\n",
        "        \n",
        "        #205x1936512 and 10368x10\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.cnn_block1(x)\n",
        "        x = self.cnn_block2(x)\n",
        "        x = self.cnn_block3(x)\n",
        "\n",
        "        # flatten the processed tensor for linear layer\n",
        "        x = x.view(x.shape[0], -1) # (batch, 128*9*9)\n",
        "        # now we can pass the data to the linear layer\n",
        "        out = self.fc_layer(x)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.cnn_block1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=2, padding=0, dilation=1, groups=1, bias=False), # output shape = (batch, 32, 13, 13)\n",
        "            nn.BatchNorm2d(num_features=32),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.cnn_block2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=0, dilation=1, groups=1, bias=False), # output shape = (batch, 64, 11, 11)\n",
        "            nn.BatchNorm2d(num_features=64),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.cnn_block3 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=0, dilation=1, groups=1, bias=False), # output shape = (batch, 128, 9, 9)\n",
        "            nn.BatchNorm2d(num_features=128),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        \n",
        "        self.fc_layer = nn.Linear(1936512, 10)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.cnn_block1(x)\n",
        "        x = self.cnn_block2(x)\n",
        "        x = self.cnn_block3(x)\n",
        "\n",
        "        # flatten the processed tensor for linear layer\n",
        "        x = x.view(x.shape[0], -1) # (batch, 128*9*9)\n",
        "        # now we can pass the data to the linear layer\n",
        "        out = self.fc_layer(x)\n",
        "        return out"
      ],
      "metadata": {
        "id": "1VhVPUFI_FSR"
      },
      "id": "1VhVPUFI_FSR",
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#torch.backends.cudnn.enabled = False\n",
        "loss_fn = nn.CrossEntropyLoss\n",
        "optimizer = torch.optim.Adam\n",
        "model_ = CNN\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "print(\"Training on \", device)\n",
        "\n",
        "training_losses, validation_losses = train_cv(model_, loss_fn, optimizer, dataset=train_dataset, device=device, epochs=10, lr=1e-3)\n",
        "\n",
        "for fold_idx, (train_losses, val_losses) in enumerate(zip(training_losses, validation_losses), 1):\n",
        "    plt.title(f'FOLD: {fold_idx}')\n",
        "    plt.plot(train_losses, label='training loss')\n",
        "    plt.plot(val_losses, label='validation loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "print('total allocated gpu memory (pytorch):', torch.cuda.memory_allocated())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 568
        },
        "id": "pp0Ww6LU_VHj",
        "outputId": "360cd645-85b3-49f3-9b4b-15a18a5a2c1b"
      },
      "id": "pp0Ww6LU_VHj",
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on  cuda:0\n",
            "CURRENT FOLD: 1\n",
            "Number of samples\n",
            "Training: 27380\n",
            "Validation: 6845\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/107 [04:41<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-380d3fab563f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training on \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtraining_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_cv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfold_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_losses\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-20b7c51791cf>\u001b[0m in \u001b[0;36mtrain_cv\u001b[0;34m(model_, loss_fn_, optimizer_, dataset, batch_size, device, cv, epochs, random_state, **opt_args)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;31m# train the model using training and validation sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         fold_training_losses, fold_validation_losses = train(model_, loss_fn_, optimizer_,\n\u001b[0m\u001b[1;32m     51\u001b[0m                                                              \u001b[0mtrain_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                                                              device=device, epochs=epochs, **opt_args)\n",
            "\u001b[0;32m<ipython-input-30-20b7c51791cf>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model_, loss_fn_, optimizer_, train_loader, val_loader, return_model, device, epochs, **opt_args)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0;31m# get the model outputs and calculate the loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n",
            "\u001b[0;32m<ipython-input-38-5077bc71f436>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcnn_block1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcnn_block2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcnn_block3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0mArguments\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0mmodules\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0miterable\u001b[0m \u001b[0mof\u001b[0m \u001b[0mmodules\u001b[0m \u001b[0mto\u001b[0m \u001b[0mappend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \"\"\"\n\u001b[1;32m    206\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontainer_abcs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    461\u001b[0m                  \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdilation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m                  bias=True, padding_mode='zeros'):\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0mkernel_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_triple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m         \u001b[0mstride\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_triple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m         \u001b[0mpadding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_triple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    457\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0m_link\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m         \u001b[0mhttps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mgithub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcom\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mvdumoulin\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mconv_arithmetic\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mblob\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mmaster\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mREADME\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m     \"\"\"\n\u001b[0m\u001b[1;32m    460\u001b[0m     def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n\u001b[1;32m    461\u001b[0m                  \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdilation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasSgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)`"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "5e477500",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 568
        },
        "id": "5e477500",
        "outputId": "cf6a997e-738b-44b7-be6f-b4956c76a41d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on  cuda:0\n",
            "CURRENT FOLD: 1\n",
            "Number of samples\n",
            "Training: 27380\n",
            "Validation: 6845\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/107 [00:50<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-05e06cc45231>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training on \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtraining_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_cv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfold_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_losses\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-20b7c51791cf>\u001b[0m in \u001b[0;36mtrain_cv\u001b[0;34m(model_, loss_fn_, optimizer_, dataset, batch_size, device, cv, epochs, random_state, **opt_args)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;31m# train the model using training and validation sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         fold_training_losses, fold_validation_losses = train(model_, loss_fn_, optimizer_,\n\u001b[0m\u001b[1;32m     51\u001b[0m                                                              \u001b[0mtrain_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                                                              device=device, epochs=epochs, **opt_args)\n",
            "\u001b[0;32m<ipython-input-30-20b7c51791cf>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model_, loss_fn_, optimizer_, train_loader, val_loader, return_model, device, epochs, **opt_args)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0;31m# get the model outputs and calculate the loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n",
            "\u001b[0;32m<ipython-input-38-5077bc71f436>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcnn_block1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcnn_block2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcnn_block3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0mArguments\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0mmodules\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0miterable\u001b[0m \u001b[0mof\u001b[0m \u001b[0mmodules\u001b[0m \u001b[0mto\u001b[0m \u001b[0mappend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \"\"\"\n\u001b[1;32m    206\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontainer_abcs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    461\u001b[0m                  \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdilation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m                  bias=True, padding_mode='zeros'):\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0mkernel_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_triple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m         \u001b[0mstride\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_triple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m         \u001b[0mpadding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_triple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    457\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0m_link\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m         \u001b[0mhttps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mgithub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcom\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mvdumoulin\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mconv_arithmetic\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mblob\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mmaster\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mREADME\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m     \"\"\"\n\u001b[0m\u001b[1;32m    460\u001b[0m     def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n\u001b[1;32m    461\u001b[0m                  \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdilation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasSgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)`"
          ]
        }
      ],
      "source": [
        "\n",
        "loss_fn = nn.CrossEntropyLoss\n",
        "optimizer = torch.optim.Adam\n",
        "model_ = CNN\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "print(\"Training on \", device)\n",
        "\n",
        "training_losses, validation_losses = train_cv(model_, loss_fn, optimizer, dataset=train_dataset, device=device, epochs=10, lr=1e-3)\n",
        "\n",
        "for fold_idx, (train_losses, val_losses) in enumerate(zip(training_losses, validation_losses), 1):\n",
        "    plt.title(f'FOLD: {fold_idx}')\n",
        "    plt.plot(train_losses, label='training loss')\n",
        "    plt.plot(val_losses, label='validation loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "print('total allocated gpu memory (pytorch):', torch.cuda.memory_allocated())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1514d099",
      "metadata": {
        "id": "1514d099",
        "outputId": "92ef0709-a823-44f1-c3c4-3f2c44d7c2ba"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r",
            "  0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of samples\n",
            "Training: 257\n",
            "Validation: 19\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [02:58<00:00, 178.36s/it]\n",
            "100%|██████████| 1/1 [00:02<00:00,  2.01s/it]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 1/10 | avg. training loss: 2.038, avg. validation loss: 5.664\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [02:32<00:00, 152.33s/it]\n",
            "100%|██████████| 1/1 [00:02<00:00,  2.64s/it]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2/10 | avg. training loss: 54.032, avg. validation loss: 6.788\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [02:35<00:00, 155.19s/it]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.86s/it]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 3/10 | avg. training loss: 40.467, avg. validation loss: 6.537\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [02:43<00:00, 163.49s/it]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.78s/it]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 4/10 | avg. training loss: 12.705, avg. validation loss: 5.198\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [02:40<00:00, 160.70s/it]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.74s/it]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 5/10 | avg. training loss: 4.436, avg. validation loss: 3.076\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [02:41<00:00, 161.84s/it]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.75s/it]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 6/10 | avg. training loss: 5.150, avg. validation loss: 1.502\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [03:25<00:00, 205.17s/it]\n",
            "100%|██████████| 1/1 [00:02<00:00,  2.85s/it]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 7/10 | avg. training loss: 2.125, avg. validation loss: 0.930\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [03:03<00:00, 183.57s/it]\n",
            "100%|██████████| 1/1 [00:02<00:00,  2.04s/it]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 8/10 | avg. training loss: 0.898, avg. validation loss: 0.192\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [02:43<00:00, 163.65s/it]\n",
            "100%|██████████| 1/1 [00:02<00:00,  2.35s/it]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 9/10 | avg. training loss: 0.509, avg. validation loss: 0.002\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [02:54<00:00, 174.06s/it]\n",
            "100%|██████████| 1/1 [00:02<00:00,  2.07s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 10/10 | avg. training loss: 0.163, avg. validation loss: 0.002\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total allocated gpu memory (pytorch): 0\n"
          ]
        }
      ],
      "source": [
        "training_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=512, shuffle=False)\n",
        "\n",
        "training_losses, validation_losses, model = train(model_, loss_fn, optimizer, \n",
        "                                                  training_loader, val_loader, device=device,\n",
        "                                                  return_model=True, epochs=10, lr=1e-3)\n",
        "\n",
        "plt.title('Performance')\n",
        "plt.plot(training_losses, label='training loss')\n",
        "plt.plot(validation_losses, label='validation loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print('total allocated gpu memory (pytorch):', torch.cuda.memory_allocated())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d97a19fc",
      "metadata": {
        "id": "d97a19fc",
        "outputId": "c703b3b2-9341-4f7f-8b08-54cfdc44475d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r",
            "  0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training report\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:41<00:00, 41.49s/it]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      1.000     0.778     0.875        18\n",
            "           1      0.984     1.000     0.992       239\n",
            "\n",
            "    accuracy                          0.984       257\n",
            "   macro avg      0.992     0.889     0.933       257\n",
            "weighted avg      0.985     0.984     0.984       257\n",
            "\n",
            "Validation report\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:01<00:00,  1.86s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      1.000     1.000     1.000         2\n",
            "           1      1.000     1.000     1.000        17\n",
            "\n",
            "    accuracy                          1.000        19\n",
            "   macro avg      1.000     1.000     1.000        19\n",
            "weighted avg      1.000     1.000     1.000        19\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"Training report\")\n",
        "train_report = evaluate(train_dataset, model, device=device, batch_size=512)\n",
        "print(\"Validation report\")\n",
        "val_report = evaluate(val_dataset, model, device=device, batch_size=512)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b719f3ea",
      "metadata": {
        "id": "b719f3ea"
      },
      "source": [
        "### Using GradCAM\n",
        "Since we already prepared a helper function for that, using GradCAM will be pretty simple."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39219ba8",
      "metadata": {
        "id": "39219ba8"
      },
      "outputs": [],
      "source": [
        "# set the layers you want to visualize\n",
        "cnn_layers = [model.cnn_block1,  model.cnn_block2, model.cnn_block3]\n",
        "\n",
        "val_dataset = val_dataset\n",
        "\n",
        "# We have to specify the target we want to generate the class activation maps for.\n",
        "# If the target is None, the highest scoring category will be used for every image in the batch.\n",
        "# I am using ClassifierOutputSoftmaxTarget, it automatically applies softmax to the model outputs.\n",
        "# it is also possible to define your own custom targets. \n",
        "\n",
        "targets = [ClassifierOutputSoftmaxTarget(i) for i in range(10)]\n",
        "\n",
        "# You can also pass aug_smooth=True and eigen_smooth=True to apply smoothing.\n",
        "gradcam(model, GradCAM, cnn_layers, targets, val_dataset, aug_smooth=True, eigen_smooth=True) #use_cuda=True)\n",
        "print('total allocated gpu memory (pytorch):', torch.cuda.memory_allocated())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40743315",
      "metadata": {
        "id": "40743315"
      },
      "outputs": [],
      "source": [
        "# You can also pass aug_smooth=True and eigen_smooth=True to apply smoothing.\n",
        "gradcam(model, GradCAM, cnn_layers, targets, val_dataset, aug_smooth=True, eigen_smooth=True) #use_cuda=True)\n",
        "print('total allocated gpu memory (pytorch):', torch.cuda.memory_allocated())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c53ef510",
      "metadata": {
        "id": "c53ef510"
      },
      "source": [
        "# Az inja:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1478c57a",
      "metadata": {
        "id": "1478c57a"
      },
      "outputs": [],
      "source": [
        "# read the training and test datasets\n",
        "DATASET_PATH = '/Users/azadehbaghdadi/Desktop/New research-CNN/Project/Data/sample/train'\n",
        "# start by importing the datasets as dataframes\n",
        "train_csv = pd.read_csv(os.path.join(DATASET_PATH, 'train.csv'))\n",
        "test_csv = pd.read_csv(os.path.join(DATASET_PATH, 'test.csv'))\n",
        "\n",
        "# split the training dataset to construct a validation set\n",
        "train_set, val_set = train_test_split(train_csv, test_size=.2)\n",
        "test_set = test_csv.copy()\n",
        "\n",
        "\n",
        "print('Train dataset:')\n",
        "display(train_set.sample(5))\n",
        "print('Validation dataset:')\n",
        "display(val_set.sample(5))\n",
        "print('Test dataset:')\n",
        "display(test_set.sample(5))\n",
        "\n",
        "print(\"Total samples in training data:\", len(train_set))\n",
        "print(\"Total samples in validation data:\", len(val_set))\n",
        "print(\"Total samples in test data:\", len(test_csv))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec444744",
      "metadata": {
        "id": "ec444744"
      },
      "outputs": [],
      "source": [
        "\n",
        "class MNISTDataset(Dataset):\n",
        "    \"\"\"Simple Dataset class for MNIST\n",
        "    \n",
        "    Simply decides if the dataset is for training or testing by checking the number of columns in the dataframe.\n",
        "    \n",
        "    Uses the`_initialize` method to reshape and define the input and target data\n",
        "    \"\"\"\n",
        "    def __init__(self, dataframe, transformations=None):\n",
        "        self._dataframe = dataframe\n",
        "        self.transformations = transformations\n",
        "        self.inputs = None\n",
        "        self.targets = None\n",
        "    \n",
        "        if dataframe.shape[1] == 785: # if it has one more column besides the pixels, it is the training set\n",
        "            self.dataset_type = 'training'\n",
        "        else:\n",
        "            self.dataset_type = 'test'\n",
        "\n",
        "        self._initialize()\n",
        "    \n",
        "    def _initialize(self):\n",
        "        if self.dataset_type == 'training':\n",
        "            self.inputs = self._dataframe.iloc[:, 1:].values.reshape(-1,1, 28, 28)\n",
        "            self.targets = self._dataframe.iloc[:, 0].values\n",
        "        else:\n",
        "            self.inputs = self._dataframe.values.reshape(-1, 1, 28, 28)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self._dataframe)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        x = self.inputs[idx]\n",
        "        if self.transformations:\n",
        "            x = self.transformations(x)\n",
        "        else:\n",
        "            x = x/255.\n",
        "            x = torch.tensor(x, dtype=torch.float32)\n",
        "        \n",
        "        \n",
        "        if self.dataset_type == 'training':\n",
        "            y = self.targets[idx]\n",
        "            y = torch.tensor(y, dtype=torch.long)\n",
        "            return x,y\n",
        "        else:\n",
        "            return x\n",
        "    \n",
        "\n",
        "\n",
        "train_dataset = MNISTDataset(train_set)\n",
        "show(train_dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78d59f03",
      "metadata": {
        "id": "78d59f03"
      },
      "outputs": [],
      "source": [
        "\n",
        "test_dataset = MNISTDataset(test_set)\n",
        "val_dataset = MNISTDataset(val_set)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2bff0cd",
      "metadata": {
        "id": "f2bff0cd"
      },
      "outputs": [],
      "source": [
        "# since the dataset is in a dataframe, we can apply the preprocessing pipeline to the dataframe\n",
        "# then we can get the dataset for training\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b705569e",
      "metadata": {
        "id": "b705569e"
      },
      "outputs": [],
      "source": [
        "show(train_loader.dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ea27a09",
      "metadata": {
        "id": "7ea27a09"
      },
      "outputs": [],
      "source": [
        "show(test_loader.dataset)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3718493",
      "metadata": {
        "id": "f3718493"
      },
      "source": [
        "# Ta inja"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4d32a73",
      "metadata": {
        "id": "e4d32a73"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}